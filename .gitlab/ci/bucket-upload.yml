.bucket-upload-rule:
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /pull\/[0-9]+/'
      when: never
    - if: '$BUCKET_UPLOAD == "true"'

.bucket-upload-rule-always:
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /pull\/[0-9]+/'
      when: never
    - if: '$BUCKET_UPLOAD == "true"'
      when: always


.check_user_permissions_to_upload_packs: &check_user_permissions_to_upload_packs
  - section_start "Check User Permissions to Upload Packs" # if bucket upload and uploading to marketplace-dist
  - |
    if [[ -n "${BUCKET_UPLOAD}" || -n "${FORCE_BUCKET_UPLOAD}" ]] && [[ "$GCS_MARKET_BUCKET" == "$GCS_PRODUCTION_BUCKET" ]]; then
      CONTENT_LEADERS=$(curl -sS "https://api.github.com/orgs/demisto/teams/content-leaders/members" -H "Authorization: token ${GITHUB_TOKEN}")
      echo "recieved content leaders"
      LEADER_NAMES=$(echo $CONTENT_LEADERS | jq -r ".[].login")
      LEADER_NAMES=$(echo "${LEADER_NAMES}" "content-bot" "svc -xsoar-gitlab-mirror" "${USERS_ALLOWED_TRIGGER_UPLOAD}" )
      if [[ -z "$GITLAB_USER_NAME" ]] || [[ -z "`echo $LEADER_NAMES | grep -w "$GITLAB_USER_NAME"`" ]]; then
        echo -e "User '$GITLAB_USER_NAME' is not allowed to trigger this build, only one of:\n${LEADER_NAMES}"
        exit 1
      else
        echo "User '${GITLAB_USER_NAME}' is allowed to upload packs / force upload packs."
      fi
    fi
  - section_end "Check User Permissions to Upload Packs"


run-validations-upload-flow:
  extends:
    - .run-validations
    - .bucket-upload-rule


run-unittests-and-lint-upload-flow:
  extends:
    - .run-unittests-and-lint
    - .bucket-upload-rule


create-instances-upload-flow:
  extends:
    - create-instances
  variables:
    IFRA_ENV_TYPE: "Bucket-Upload"
    ARTIFACTS_FOLDER: "${ARTIFACTS_FOLDER_XSOAR}"
  rules:
    - if: '$CI_COMMIT_BRANCH =~ /pull\/[0-9]+/'
      when: never
    - if: '$BUCKET_UPLOAD == "true"'
    - if: '$FORCE_BUCKET_UPLOAD == "true"'

prepare-testing-bucket-mpv2-upload-flow:
 extends:
   - prepare-testing-bucket-mpv2
 variables:
   IFRA_ENV_TYPE: "Bucket-Upload"
   ARTIFACTS_FOLDER: "${ARTIFACTS_FOLDER_MPV2}"
 rules:
   - if: '$CI_COMMIT_BRANCH =~ /pull\/[0-9]+/'
     when: never
   - if: '$BUCKET_UPLOAD == "true"'
   - if: '$FORCE_BUCKET_UPLOAD == "true"'


.install_packs_in_server:
  tags:
    - gke
  needs: ["create-instances-upload-flow"]
  stage: run-instances
  artifacts:
    expire_in: 48 hrs
    paths:
      - /builds/xsoar/content/artifacts/*
    when: always
  variables:
    ARTIFACTS_FOLDER: "${ARTIFACTS_FOLDER_XSOAR}"
    ENV_RESULTS_PATH: "${ARTIFACTS_FOLDER_XSOAR}/env_results.json"
    INSTANCE_CREATED: "true"
    SSH_TUNNEL_TIMEOUT: 10
    TIME_TO_LIVE: ""
  extends:
    - .bucket-upload-rule
    - .default-job-settings
  script:
    - EXIT_CODE=0
    - section_start "Download configuration"
    - ./Tests/scripts/download_demisto_conf.sh  >> $ARTIFACTS_FOLDER/logs/installations.log
    - section_end "Download configuration"
    - !reference [.open-ssh-tunnel]
    - section_start "Get Instance Variables"
    - echo INSTANCE_ROLE="$INSTANCE_ROLE"
    - echo INSTANCE_CREATED="$INSTANCE_CREATED"
    - section_end "Get Instance Variables"
    - section_start "Wait Until Server Ready"
    - python3 ./Tests/scripts/wait_until_server_ready.py "$INSTANCE_ROLE" || EXIT_CODE=$?
    - section_end "Wait Until Server Ready"
    - section_start "Install Packs"
    - ./Tests/Marketplace/install_packs.sh "$INSTANCE_ROLE" || EXIT_CODE=$?
    - section_end "Install Packs"
    - section_start "Destroy instances"
    - python3 ./Tests/scripts/destroy_instances.py $ARTIFACTS_FOLDER $ARTIFACTS_FOLDER/env_results.json "$INSTANCE_ROLE" "$TIME_TO_LIVE" || EXIT_CODE=$?
    - section_end "Destroy instances"
    - exit "$EXIT_CODE"


install-packs-in-server6_2:
  extends: .install_packs_in_server
  variables:
    INSTANCE_ROLE: "Server 6.2"


install-packs-in-server6_1:
  extends: .install_packs_in_server
  variables:
    INSTANCE_ROLE: "Server 6.1"


install-packs-in-server-master:
  extends: .install_packs_in_server
  variables:
    INSTANCE_ROLE: "Server Master"


# TO-DO: Job to test installation of mpv2 packs - issue https://github.com/demisto/etc/issues/44769
# install-mpv2-pack-on-mpv2-instances


upload-packs-to-marketplace:
  tags:
    - gke
  needs: ["run-validations-upload-flow", "install-packs-in-server6_2", "install-packs-in-server6_1", "install-packs-in-server-master", "run-unittests-and-lint-upload-flow"]
  stage: upload-to-marketplace
  artifacts:
    expire_in: 48 hrs
    paths:
      - /builds/xsoar/content/artifacts/*
    when: always
  variables:
    ARTIFACTS_FOLDER: "${ARTIFACTS_FOLDER_XSOAR}"
    ENV_RESULTS_PATH: "${ARTIFACTS_FOLDER_XSOAR}/env_results.json"
    INSTANCE_ROLE: "Server Master"
    INSTANCE_CREATED: "true"
    SSH_TUNNEL_TIMEOUT: 10
    TIME_TO_LIVE: ""
  extends:
    - .bucket-upload-rule
    - .default-job-settings
  script:
    - !reference [.open-ssh-tunnel]
    - *check_user_permissions_to_upload_packs
    - section_start "Upload Packs To Marketplace Storage"
    - |
      if [[ "$CI_COMMIT_BRANCH" == "master" ]] || [[ "$GCS_MARKET_BUCKET" != "$GCS_PRODUCTION_BUCKET" ]]; then
        EXTRACT_FOLDER=$(mktemp -d)
        PACK_ARTIFACTS=$ARTIFACTS_FOLDER/content_packs.zip
        PACKS_DEPENDENCIES=$ARTIFACTS_FOLDER/packs_dependencies.json
        CI_COMMIT_BRANCH=${CI_COMMIT_BRANCH:-unknown}
        GCS_BUILD_BUCKET="marketplace-ci-build"
        if [[ $GCS_MARKET_BUCKET == $GCS_PRODUCTION_BUCKET ]] && [[ -z $STORAGE_BASE_PATH ]]; then
          STORAGE_BASE_PATH="content"
        fi

        if [[ $GCS_MARKET_BUCKET != $GCS_PRODUCTION_BUCKET ]] && [[ -z $STORAGE_BASE_PATH ]]; then
         STORAGE_BASE_PATH="upload-flow/builds/$CI_COMMIT_BRANCH/$CI_PIPELINE_ID/content"
        fi

        python3 ./Tests/Marketplace/copy_and_upload_packs.py -a $PACK_ARTIFACTS -e $EXTRACT_FOLDER -pb "$GCS_MARKET_BUCKET" -bb "$GCS_BUILD_BUCKET" -s $GCS_MARKET_KEY -n $CI_PIPELINE_ID -c $CI_COMMIT_BRANCH -pbp "$STORAGE_BASE_PATH/packs" --marketplace xsoar
      fi
    - section_end "Upload Packs To Marketplace Storage"

    - section_start "Download packs from GCP"
    - |
      if [[ $GCS_MARKET_BUCKET == $GCS_PRODUCTION_BUCKET ]] && [[ -z $STORAGE_BASE_PATH ]]; then
        STORAGE_BASE_PATH="content"
      fi
      if [[ $GCS_MARKET_BUCKET != $GCS_PRODUCTION_BUCKET ]] && [[ -z $STORAGE_BASE_PATH ]]; then
        STORAGE_BASE_PATH="upload-flow/builds/$CI_COMMIT_BRANCH/$CI_PIPELINE_ID/content"
      fi

      PACKS_SRC="gs://$GCS_MARKET_BUCKET/$STORAGE_BASE_PATH/packs"
      ZIP_FOLDER=$(mktemp -d)
      gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" >> $ARTIFACTS_FOLDER/logs/auth.out
      echo "successfully activated google cloud service account"
      gsutil -m cp -r $PACKS_SRC $ZIP_FOLDER
      echo "successfully downloaded index.zip"
      gcloud auth revoke $GCS_ARTIFACTS_ACCOUNT_NAME
    - section_end "Download packs from GCP"

    - section_start "Zip Content Packs From GCS"
    - python3 ./Tests/Marketplace/zip_packs.py -z $ZIP_FOLDER -a $ARTIFACTS_FOLDER -s $GCS_MARKET_KEY
    - section_end "Zip Content Packs From GCS"

    - section_start "Validate Premium Packs"
    - |
      if [[ "$CI_COMMIT_BRANCH" == "master" ]] || [[ $GCS_MARKET_BUCKET != $GCS_PRODUCTION_BUCKET ]]; then
        ./Tests/scripts/validate_premium_packs.sh "$INSTANCE_ROLE"
      else
        echo "Skipping Premium Packs Validation"
      fi
    - section_end "Validate Premium Packs"


upload-packs-to-marketplace-v2:
  tags:
    - gke
  needs: ["run-validations-upload-flow", "run-unittests-and-lint-upload-flow", "prepare-testing-bucket-mpv2-upload-flow"]
  stage: upload-to-marketplace
  artifacts:
    expire_in: 48 hrs
    paths:
      - /builds/xsoar/content/artifacts/*
    when: always
  variables:
    ARTIFACTS_FOLDER: "${ARTIFACTS_FOLDER_MPV2}"
    INSTANCE_ROLE: "Server Master"
    INSTANCE_CREATED: "true"
    SSH_TUNNEL_TIMEOUT: 10
    TIME_TO_LIVE: ""
  extends:
    - .bucket-upload-rule
    - .default-job-settings
  script:
    - *check_user_permissions_to_upload_packs
    - section_start "Upload Packs To Marketplace Storage"
    - |
      if [[ "$CI_COMMIT_BRANCH" == "master" ]] || [[ "$GCS_MARKET_V2_BUCKET" != "$GCS_PRODUCTION_V2_BUCKET" ]]; then
        EXTRACT_FOLDER=$(mktemp -d)
        PACK_ARTIFACTS=$ARTIFACTS_FOLDER/content_packs.zip
        PACKS_DEPENDENCIES=$ARTIFACTS_FOLDER/packs_dependencies.json
        CI_COMMIT_BRANCH=${CI_COMMIT_BRANCH:-unknown}
        GCS_BUILD_BUCKET="marketplace-ci-build"
        if [[ -z $STORAGE_BASE_PATH ]]; then
          if [[ $GCS_MARKET_V2_BUCKET == $GCS_PRODUCTION_V2_BUCKET ]]; then
            STORAGE_BASE_PATH="content"
          else
            STORAGE_BASE_PATH="upload-flow/builds/$CI_COMMIT_BRANCH/$CI_PIPELINE_ID/content"
          fi
          echo "Set storage base path to $STORAGE_BASE_PATH"
        fi
        python3 ./Tests/Marketplace/copy_and_upload_packs.py -a $PACK_ARTIFACTS -e $EXTRACT_FOLDER -pb "$GCS_MARKET_V2_BUCKET" -bb "$GCS_BUILD_BUCKET" -s $GCS_MARKET_KEY -n $CI_PIPELINE_ID -c $CI_COMMIT_BRANCH -pbp "$STORAGE_BASE_PATH/packs" --marketplace marketplacev2
      fi
    - section_end "Upload Packs To Marketplace Storage"

    - section_start "Download packs from GCP"
    - |
      echo "$STORAGE_BASE_PATH"
      PACKS_SRC="gs://$GCS_MARKET_V2_BUCKET/$STORAGE_BASE_PATH/packs"
      ZIP_FOLDER=$(mktemp -d)
      gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" >> $ARTIFACTS_FOLDER/logs/auth.out
      echo "successfully activated google cloud service account"
      gsutil -m cp -r $PACKS_SRC $ZIP_FOLDER
      echo "successfully downloaded index.zip"
      gcloud auth revoke $GCS_ARTIFACTS_ACCOUNT_NAME
    - section_end "Download packs from GCP"

    - section_start "Zip Content Packs From GCS"
    - python3 ./Tests/Marketplace/zip_packs.py -z $ZIP_FOLDER -a $ARTIFACTS_FOLDER -s $GCS_MARKET_KEY
    - section_end "Zip Content Packs From GCS"



force-pack-upload:
  tags:
    - gke
  stage: upload-to-marketplace
  needs: ["create-instances-upload-flow"]
  rules:
    - if: '$FORCE_BUCKET_UPLOAD == "true"'
  extends:
    - .default-job-settings
  script:
    - *check_user_permissions_to_upload_packs
    - EXTRACT_FOLDER=$(mktemp -d)
    - PACK_ARTIFACTS=$ARTIFACTS_FOLDER/content_packs.zip
    - PACKS_DEPENDENCIES=$ARTIFACTS_FOLDER/packs_dependencies.json
    - CI_COMMIT_BRANCH=${CI_COMMIT_BRANCH:-unknown}
    - GCS_BUILD_BUCKET="marketplace-ci-build"
    - |
      if [[ $GCS_MARKET_BUCKET == $GCS_PRODUCTION_BUCKET ]] && [[ -z $STORAGE_BASE_PATH ]]; then
        STORAGE_BASE_PATH="content"
      fi

      if [[ $GCS_MARKET_BUCKET != $GCS_PRODUCTION_BUCKET ]] && [[ -z $STORAGE_BASE_PATH ]]; then
       STORAGE_BASE_PATH="upload-flow/builds/$CI_COMMIT_BRANCH/$CI_PIPELINE_ID/content"
      fi

    - python3 ./Tests/Marketplace/copy_and_upload_packs.py -a $PACK_ARTIFACTS -e $EXTRACT_FOLDER -pb "$GCS_MARKET_BUCKET" -bb "$GCS_BUILD_BUCKET" -s $GCS_MARKET_KEY -n $CI_PIPELINE_ID -c $CI_COMMIT_BRANCH -p "${PACKS_TO_UPLOAD}" -pbp "$STORAGE_BASE_PATH/packs"


fan-in-bucket-upload:
  tags:
    - gke
  stage: fan-in
  extends:
    - .bucket-upload-rule-always
  script:
    - echo "fan in"


slack-notify-bucket-upload:
  variables:
    PIPELINE_TO_QUERY: $CI_PIPELINE_ID
    WORKFLOW: 'Upload Packs to Marketplace Storage'
    JOB_NAME: 'fan-in-bucket-upload'
    # Passes the environment variable from the parent pipeline to the child which can be useful for cases
    # when triggering pipeline with alternate env variable value passed in the API call
    SLACK_CHANNEL: $SLACK_CHANNEL
  extends:
    - .trigger-slack-notification
    - .bucket-upload-rule-always
