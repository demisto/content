.default-cache:
  cache:
    key:
      files:
        - "poetry.lock"
        - "package-lock.json"
      prefix: dev-content
    paths:
      - $PIP_CACHE_DIR
      - .venv/
      - node_modules/
      - .npm/
    policy: pull


.setup-network-certs: &setup-network-certs
  - source .gitlab/helper_functions.sh
  - chmod 700 $NETWORK_SETUP
  - source $NETWORK_SETUP
  - section_end "Setup network certs"


### Global Script Snippets ###

.create-id-set: &create-id-set
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - section_end "Create ID Set"

.create-id-set-xsoar: &create-id-set-xsoar
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xsoar" >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - section_end "Create ID Set"

.create-id-set-mp-v2: &create-id-set-mp-v2
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "marketplacev2" >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - section_end "Create ID Set"

.create-id-set-xpanse: &create-id-set-xpanse
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xpanse" >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - section_end "Create ID Set"

.download-demisto-conf:
  - section_start "Download content-test-conf" --collapsed
  - ./Tests/scripts/download_conf_repos.sh  >> $ARTIFACTS_FOLDER/logs/download_demisto_conf.log
  - SECRET_CONF_PATH=$(cat secret_conf_path)
  - python3 ./Tests/scripts/add_secrets_file_to_build.py -sa "$GSM_SERVICE_ACCOUNT" -sf "$SECRET_CONF_PATH" -u "$DEMISTO_USERNAME" -p "$DEMISTO_PASSWORD" -gpid "$GSM_PROJECT_ID"
  - section_end "Download content-test-conf"

.ssh-config-setup:
  - section_start "SSH config setup" --collapsed
  - cp $GCP_SSH_CONFIGURATION ~/.ssh/config
  - chmod 700 ~/.ssh/config
  - section_end "SSH config setup"

.check_build_files_are_up_to_date: &check_build_files_are_up_to_date
  - section_start "Check Build Files Are Up To Date"
  - |
    if [[ -n "${DEMISTO_SDK_NIGHTLY}" || -n "${NIGHTLY}" || -n "${BUCKET_UPLOAD}" || -n "${SLACK_JOB}" ]]; then
      echo "running nightly or bucket upload, skipping files up-to-date validation"
    else
      ./Tests/scripts/is_file_up_to_date.sh .gitlab $CI_COMMIT_BRANCH
      # we want to checkout if it's not up-to-date
      ./Tests/scripts/is_file_up_to_date.sh poetry.lock $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh pyproject.toml $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_mpv2_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_xpanse_list.json $CI_COMMIT_BRANCH true
    fi
  - section_end "Check Build Files Are Up To Date"


.clone_and_export_variables: &clone_and_export_variables
  - source .gitlab/helper_functions.sh
  - section_start "Git - Job Start Actions" --collapsed
  - git checkout -B $CI_COMMIT_BRANCH $CI_COMMIT_SHA
  - git config diff.renameLimit 6000
  - section_end "Git - Job Start Actions"
  - mkdir -p -m 777 $ARTIFACTS_FOLDER/
  - |
    if [[ -f "$BASH_ENV" ]]; then
      source "$BASH_ENV"
    fi
  - source .circleci/content_release_vars.sh
  - section_start "Granting execute permissions on files" --collapsed
  - chmod +x ./Tests/scripts/*
  - chmod +x ./Tests/Marketplace/*
  - section_end "Granting execute permissions on files"

.get_contribution_pack: &get_contribution_pack
  - section_start "getting contrib packs" --collapsed
  - |
    if [[ -n "${CONTRIB_BRANCH}" ]]; then
      REPO=$(echo $CONTRIB_BRANCH | cut -d ":" -f 1)
      BRANCH=$(echo $CONTRIB_BRANCH | cut -d ":" -f 2)
      python3 ./Utils/update_contribution_pack_in_base_branch.py -p $PULL_REQUEST_NUMBER -b $BRANCH -c $REPO
    fi
  - section_end "getting contrib packs"

.install_venv: &install_venv
  - section_start "Installing Virtualenv" --collapsed
  - echo "Checking if pyproject.toml is consistent with poetry.lock"
  - poetry lock --check
  # we still need to install even if cached. if cached, `poetry` will handle it
  - echo "installing venv"
  - NO_HOOKS=1 .hooks/bootstrap | tee --append $ARTIFACTS_FOLDER/logs/installations.log
  - npm ci --cache .npm --prefer-offline | tee --append $ARTIFACTS_FOLDER/logs/installations.log
  - source ./.venv/bin/activate
  - |
    if [[ -n "${DEMISTO_SDK_NIGHTLY}" || -n "${OVERRIDE_SDK_REF}" ]]; then
      echo "Installing SDK from $SDK_REF" | tee --append $ARTIFACTS_FOLDER/logs/installations.log
      pip3 uninstall -y demisto-sdk | tee --append $ARTIFACTS_FOLDER/logs/installations.log
      pip3 install "git+https://github.com/demisto/demisto-sdk@${SDK_REF}#egg=demisto-sdk" | tee --append $ARTIFACTS_FOLDER/logs/installations.log
    fi
  - |
      python3 --version | tee -a $ARTIFACTS_FOLDER/installed_python_libraries.txt
      python3 -m pip list | tee -a $ARTIFACTS_FOLDER/installed_python_libraries.txt
  - section_end "Installing Virtualenv"

.install_ssh_keys: &install_ssh_keys
  - section_start "Installing SSH keys" --collapsed
  - eval $(ssh-agent -s)
  - chmod 400 $OREGON_CI_KEY
  - ssh-add $OREGON_CI_KEY
  - mkdir -p ~/.ssh
  - chmod 700 ~/.ssh
  - section_end "Installing SSH keys"

.install_node_modules: &install_node_modules
  - section_start "Installing node modules" --collapsed
  - source $NVM_DIR/nvm.sh
  - nvm use default
  - echo "Installing Node Modules" | tee --append $ARTIFACTS_FOLDER/logs/installations.log
  - npm ci --cache .npm --prefer-offline | tee --append $ARTIFACTS_FOLDER/logs/installations.log
  - npm list --json
  - npm link jsdoc-to-markdown@5.0.3 | tee --append $ARTIFACTS_FOLDER/logs/installations.log  # disable-secrets-detection
  - section_end "Installing node modules"

.get_last_upload_commit: &get_last_upload_commit
  - section_start "Getting last bucket upload commit"
  - gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" > $ARTIFACTS_FOLDER/logs/gauth.out 2>$ARTIFACTS_FOLDER/logs/gauth.err
  - gsutil cp "gs://$GCS_MARKET_BUCKET/content/packs/index.json" "$ARTIFACTS_FOLDER/previous_index.json"
  - export LAST_UPLOAD_COMMIT=$(cat $ARTIFACTS_FOLDER/previous_index.json | jq -r ".\"commit\"")
  - section_end "Getting last bucket upload commit"

.default-before-script:
  before_script:
    - *setup-network-certs
    - *clone_and_export_variables
    - *check_build_files_are_up_to_date
    - section_start "Creating new clean logs folder" --collapsed
    - rm -rf $ARTIFACTS_FOLDER/logs
    - mkdir -p $ARTIFACTS_FOLDER/logs
    - section_end "Creating new clean logs folder"
    - *install_node_modules
    - *install_venv
    - *get_contribution_pack
    - *get_last_upload_commit
    - *install_ssh_keys
    - section_start "Build Parameters"
    - set | grep -E "^NIGHTLY=|^INSTANCE_TESTS=|^SERVER_BRANCH_NAME=|^ARTIFACT_BUILD_NUM=|^DEMISTO_SDK_NIGHTLY=|^TIME_TO_LIVE=|^CONTRIB_BRANCH=|^FORCE_PACK_UPLOAD=|^PACKS_TO_UPLOAD=|^BUCKET_UPLOAD=|^STORAGE_BASE_PATH=|^OVERRIDE_ALL_PACKS=|^GCS_MARKET_BUCKET=|^GCS_MARKET_V2_BUCKET=|^GCS_MARKET_XPANSE_BUCKET=|^SLACK_CHANNEL=|^NVM_DIR=|^NODE_VERSION=|^PATH=|^ARTIFACTS_FOLDER=|^ENV_RESULTS_PATH=|^LAST_UPLOAD_COMMIT="
    - neo4j-admin dbms set-initial-password contentgraph
    - neo4j start
    - python --version
    - python3 --version
    - pip3 --version
    - node --version
    - npm --version
    - jsdoc2md --version
    - demisto-sdk --version
    - section_end "Build Parameters"

.default-job-settings:
  interruptible: true
  extends:
    - .default-cache
    - .default-before-script


.trigger-slack-notification:
  stage: .post
  trigger:
    strategy: depend
    include:
      - local: .gitlab/ci/slack-notify.yml


.unittests-and-lint-settings:
  tags:
    - gce
  needs: [ ]
  stage: unittests-and-validations
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: /builds/xsoar/content/artifacts/coverage_report/coverage.xml
    expire_in: 30 days
    paths:
      - /builds/xsoar/content/unit-tests
      - /builds/xsoar/content/artifacts/*
      - /builds/xsoar/content/pipeline_jobs_folder/*
    when: always
  services:
    - name: docker.art.code.pan.run/build-tools--image-dind:20.10.12-dind
      alias: docker
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: ""
  extends:
    - .default-job-settings


.run-unittests-and-lint:
  extends:
    - .unittests-and-lint-settings
  script:
    - section_start "Test Infrastructure"
    - python3 -m pytest ./Tests/scripts/infrastructure_tests/ -v
    - python3 -m pytest ./Tests/Marketplace/Tests/ -v
    - python3 -m pytest ./Tests/tests -v
    - python3 -m pytest ./Tests/private_build/ -v
    - python3 -m pytest Utils -v
    - |
      if [ -n "${DEMISTO_SDK_NIGHTLY}" ]; then
        ./Tests/scripts/sdk_pylint_check.sh
      fi
    - section_end "Test Infrastructure"
    - section_start "Run Unit Testing and Lint"
    - |
      if [[ -n $FORCE_BUCKET_UPLOAD || -n $BUCKET_UPLOAD ]] && [[ "$(echo "$GCS_MARKET_BUCKET" | tr '[:upper:]' '[:lower:]')" != "marketplace-dist" ]] && [[ $CI_COMMIT_BRANCH != "master" ]]; then
        echo "Skipping validations when uploading to a test bucket."
      else
        echo "demisto-sdk version: $(demisto-sdk --version)"
        echo "mypy version: $(mypy --version)"
        echo "flake8 py3 version: $(python3 -m flake8 --version)"
        echo "bandit py3 version: $(python3 -m bandit --version 2>&1)"
        echo "vulture py3 version: $(python3 -m vulture --version 2>&1)"
        SHOULD_LINT_ALL=$(./Tests/scripts/should_lint_all.sh)
        mkdir ./unit-tests
        if [[ -n "${SHOULD_LINT_ALL}" || -n "${BUCKET_UPLOAD}" ]]; then
          echo "In should lint all / bucket upload."
          if [ $DOCKER == "native:dev,from-yml" ]; then
            echo "removing native:dev from docker list."
            DOCKER='from-yml'
          fi
        fi
        if [ -n "$SHOULD_LINT_ALL" ]; then
          echo -e  "----------\nLinting all because:\n${SHOULD_LINT_ALL}\n----------"
          demisto-sdk lint -vvv -p 10 -a --test-xml ./unit-tests --log-path $ARTIFACTS_FOLDER --failure-report $ARTIFACTS_FOLDER --coverage-report $ARTIFACTS_FOLDER/coverage_report -dt 120 --time-measurements-dir $ARTIFACTS_FOLDER --docker-image ${DOCKER}
        fi
        if [[ -n $BUCKET_UPLOAD ]]; then
          demisto-sdk lint -vvv -p 8 -g --no-mypy --prev-ver $LAST_UPLOAD_COMMIT -v --test-xml ./unit-tests --log-path $ARTIFACTS_FOLDER --failure-report $ARTIFACTS_FOLDER --coverage-report $ARTIFACTS_FOLDER/coverage_report -cdam --docker-image ${DOCKER}
        else
          if [[ "$CI_COMMIT_BRANCH" != "master" ]]; then
            # skip running `lint -g` on master
            demisto-sdk lint -p 8 -g -vvv --test-xml ./unit-tests --log-path ./artifacts --failure-report ./artifacts --coverage-report $ARTIFACTS_FOLDER/coverage_report -cdam --docker-image ${DOCKER}
          fi
        fi
      
        if [[ -f $ARTIFACTS_FOLDER/coverage_report/.coverage ]]; then
          if [[ "$CI_PIPELINE_SOURCE" == "schedule" ||   -n "$SHOULD_LINT_ALL"  ||  -n "${NIGHTLY}" || -n "${BUCKET_UPLOAD}" || -n "${DEMISTO_SDK_NIGHTLY}" ]]; then
            demisto-sdk coverage-analyze -i $ARTIFACTS_FOLDER/coverage_report/.coverage --report-dir $ARTIFACTS_FOLDER/coverage_report --report-type all --allowed-coverage-degradation-percentage 100
            if [[ -n "${NIGHTLY}" && "$CI_COMMIT_BRANCH" == "master" && $DOCKER == "from-yml" ]]; then
              python3 Utils/upload_code_coverage_report.py --service_account $GCS_MARKET_KEY --source_file_name $ARTIFACTS_FOLDER/coverage_report/coverage.json --minimal_file_name $ARTIFACTS_FOLDER/coverage_report/coverage-min.json
            fi
          else
            demisto-sdk coverage-analyze -i $ARTIFACTS_FOLDER/coverage_report/.coverage --report-dir $ARTIFACTS_FOLDER/coverage_report --report-type html,xml --previous-coverage-report-url https://storage.googleapis.com/marketplace-dist-dev/code-coverage-reports/coverage-min.json
          fi
        fi
      fi
    - section_end "Run Unit Testing and Lint"
    - job-done
  parallel:
    matrix:
      - DOCKER: ['native:ga,native:maintenance,native:candidate','native:dev,from-yml']

.run-validations:
  stage: unittests-and-validations
  extends:
    - .default-job-settings
  needs: []
  variables:
    KUBERNETES_CPU_REQUEST: 1000m
  artifacts:
    expire_in: 30 days
    paths:
      - /builds/xsoar/content/artifacts/*
      - /builds/xsoar/content/pipeline_jobs_folder/*
    when: always
  script:
    - section_start "Look For Secrets"
    - demisto-sdk secrets --post-commit --ignore-entropy
    - section_end "Look For Secrets"
    - section_start "Copy Tests To Artifact Folder"
    - cp "./Tests/conf.json" "$ARTIFACTS_FOLDER/conf.json"
    - section_end "Copy Tests To Artifact Folder"
    - section_start "Validate Files and Yaml"
    - |
      if [[ -n $FORCE_BUCKET_UPLOAD ]]; then
        echo "Skipping the -Validate Files and Yaml- step when force uploading to a bucket."
      else
        ./Tests/scripts/linters_runner.sh
        ./Tests/scripts/validate.sh
      fi
    - section_end "Validate Files and Yaml"
    - section_start "Check Spelling"
    - python3 ./Tests/scripts/circleci_spell_checker.py $CI_COMMIT_BRANCH
    - section_end "Check Spelling"
    - section_start "Validate content-test-conf Branch Merged"
    - |
      if [[ $CI_COMMIT_BRANCH = "master" ]]; then
        echo "Skipping, Should not run on master branch."
      else
        # replace slashes ('/') in the branch name, if exist, with underscores ('_')
        UNDERSCORE_CI_BRANCH=${CI_COMMIT_BRANCH//\//_}
        wget --header "Accept: application/vnd.github.v3.raw" --header "Authorization: token $GITHUB_TOKEN" "https://github.com/demisto/content-test-conf/archive/$UNDERSCORE_CI_BRANCH.zip" --no-check-certificate -q || {
          if [ "$?" != "0" ]; then
            echo "No such branch in content-test-conf: $UNDERSCORE_CI_BRANCH"
          else
            echo "ERROR: Found a branch with the same name in contest-test-conf conf.json - $UNDERSCORE_CI_BRANCH.\n Merge it in order to merge the current branch into content repo."
            job-done
            exit 1
          fi
        }
      fi
    - section_end "Validate content-test-conf Branch Merged"
    - section_start "Validate landingPageSections.json"
    - echo "Download index.zip"
    - INDEX_PATH=$(mktemp)
    - |
      gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" >> $ARTIFACTS_FOLDER/logs/auth.out
      echo "successfully activated google cloud service account"
      gsutil cp "gs://marketplace-dist/content/packs/index.zip" $INDEX_PATH
      echo "successfully downloaded index.zip"
      gcloud auth revoke $GCS_ARTIFACTS_ACCOUNT_NAME
    - echo "successfully downloaded index.zip into $INDEX_PATH"

    - UNZIP_PATH=$(mktemp -d)
    - unzip $INDEX_PATH -d $UNZIP_PATH > $ARTIFACTS_FOLDER/logs/unzip_index.log

    - python3 Tests/Marketplace/validate_landing_page_sections.py -i $UNZIP_PATH
    - section_end "Validate landingPageSections.json"
    - job-done

.jobs-done-check:
  stage: are-jobs-really-done
  script:
    - python3 Tests/scripts/check_jobs_done.py --job-done-files $PIPELINE_JOBS_FOLDER
