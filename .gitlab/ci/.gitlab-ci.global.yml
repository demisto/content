.default-cache:
  cache:
    key:
      files:
        - "poetry.lock"
        - "package-lock.json"
      prefix: dev-content
    paths:
      - $PIP_CACHE_DIR
      - .venv/
      - node_modules/
      - .npm/
    policy: pull


.setup-network-certs: &setup-network-certs
  - section_start "Setup network certs" --collapsed
  - chmod 700 ${CERTIFICATE_SETUP_SCRIPT}
  - source ${CERTIFICATE_SETUP_SCRIPT}
  - section_end "Setup network certs"

.setup-artifactory: &setup-artifactory
  - section_start "Setup Artifactory" --collapsed
  - chmod 700 ${ARTIFACTORY_SETUP_SCRIPT}
  - source ${ARTIFACTORY_SETUP_SCRIPT}
  - section_end "Setup Artifactory"

### Global Script Snippets ###

.create-id-set:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - section_end "Create ID Set"

.create-id-set-xsoar:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xsoar" >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - section_end "Create ID Set"

.create-id-set-mp-v2:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "marketplacev2" >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - section_end "Create ID Set"

.create-id-set-xpanse:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xpanse" >> "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs/create_id_set.log"
  - cp ./Tests/id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}"
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs"; fi
  - section_end "Create ID Set"

.download-demisto-conf:
  - section_start "Download content-test-conf and infra" --collapsed
  - ./Tests/scripts/download_conf_repos.sh 2>&1 | tee --append "${ARTIFACTS_FOLDER}/logs/download_conf_repos.log"
  - section_end "Download content-test-conf and infra"

.secrets-fetch:
  - section_start "Secrets Fetch" --collapsed
  - SECRET_CONF_PATH=$(cat secret_conf_path)
  - python3 ./Tests/scripts/add_secrets_file_to_build.py -sa "$GSM_SERVICE_ACCOUNT" -sf "$SECRET_CONF_PATH" -u "$DEMISTO_USERNAME" -p "$DEMISTO_PASSWORD" --gsm_project_id_dev "$GSM_PROJECT_ID_DEV" --gsm_project_id_prod "$GSM_PROJECT_ID" >> "${ARTIFACTS_FOLDER}/logs/handle_secrets.log"
  - section_end "Secrets Fetch"

.check_build_files_are_up_to_date: &check_build_files_are_up_to_date
  - section_start "Check Build Files Are Up To Date"
  - |
    if [[ -n "${DEMISTO_SDK_NIGHTLY}" ]] || [[ -n "${NIGHTLY}" ]] || [[ -n "${BUCKET_UPLOAD}" ]] || [[ -n "${SLACK_JOB}" ]] || [[ "${BUILD_MACHINES_CLEANUP}" == "true" ]] || [[ "${DELETE_MISMATCHED_BRANCHES}" == "true" ]] || [[ "${SECURITY_SCANS}" == "true" ]] || [[ "${DEMISTO_TEST_NATIVE_CANDIDATE}" == "true" ]] || [[ "${CI_COMMIT_BRANCH}" == "master" ]] || [[ "${SDK_RELEASE}" == "true" ]]; then
      echo "Running a build which doesn't require build files check validation"
    else
      ./Tests/scripts/is_file_up_to_date.sh .gitlab $CI_COMMIT_BRANCH
      # we want to checkout if it's not up-to-date
      ./Tests/scripts/is_file_up_to_date.sh poetry.lock $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh pyproject.toml $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_mpv2_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_xpanse_list.json $CI_COMMIT_BRANCH true
    fi
  - section_end "Check Build Files Are Up To Date"

.stop_contrib_external_build: &stop_contrib_external_build
  - - section_start "Stop contrib external build"
  - |
    if [[ $CI_COMMIT_BRANCH =~ ^contrib/* && $CI_PIPELINE_SOURCE = "push" && $(curl --get --header "Accept: application/vnd.github.v3.raw" --header "Authorization: token $GITHUB_TOKEN" "https://api.github.com/repos/demisto/content/pulls?state=open&base=master" --data-urlencode "head=demisto:${CI_COMMIT_BRANCH}" | jq) = '[]' ]]; then
      echo "not running on contrib/ branches when there is no open internal PR or the pipeline was manually triggered"
      set -e
      exit 1
    fi
  - section_end "Stop contrib external build"

.create_artifacts_and_server_type_instance_folders: &create_artifacts_and_server_type_instance_folders
  - section_start "Create Artifacts, Server Instance, Server Type folders" --collapsed
  - |
    if [[ -n "${ARTIFACTS_FOLDER}" ]] && [[ ! -d "${ARTIFACTS_FOLDER}/logs" ]]; then
      echo "Creating Artifacts folder: ${ARTIFACTS_FOLDER} and it's log folder"
      mkdir -p -m 777 "${ARTIFACTS_FOLDER}/logs" # using the -p to create the logs folder as well.
    fi
  - |
    if [[ -n "${ARTIFACTS_FOLDER_INSTANCE}" ]] && [[ ! -d "${ARTIFACTS_FOLDER_INSTANCE}/logs" ]]; then
      echo "Creating Artifacts instance folder: ${ARTIFACTS_FOLDER_INSTANCE} and it's log folder"
      mkdir -p -m 777 "${ARTIFACTS_FOLDER_INSTANCE}/logs" # using the -p to create the logs folder as well.
      echo "${INSTANCE_ROLE}" > "${ARTIFACTS_FOLDER_INSTANCE}/instance_role.txt"
    fi
  - |
    if [[ -n "${ARTIFACTS_FOLDER_SERVER_TYPE}" ]] && [[ ! -d "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs" ]]; then
      echo "Creating Artifacts Server type folder: ${ARTIFACTS_FOLDER_SERVER_TYPE} and it's log folder"
      mkdir -p -m 777 "${ARTIFACTS_FOLDER_SERVER_TYPE}/logs" # using the -p to create the logs folder as well.
      echo "${SERVER_TYPE}" > "${ARTIFACTS_FOLDER_SERVER_TYPE}/server_type.txt"
    fi
  - section_end "Create Artifacts, Server Instance, Server Type folders"

.clone_and_export_variables: &clone_and_export_variables
  - section_start "Git - Job Start Actions" --collapsed
  - git fetch origin master:refs/remotes/origin/master
  - git checkout -B $CI_COMMIT_BRANCH $CI_COMMIT_SHA
  - git config diff.renameLimit 6000
  - section_end "Git - Job Start Actions"
  - section_start "Source BASH Environment" --collapsed
  - |
    if [[ -f "$BASH_ENV" ]]; then
      source "$BASH_ENV"
    fi
  - source .circleci/content_release_vars.sh
  # DEMISTO_SDK_GRAPH_FORCE_CREATE set to true to create graph from scratch.
  - |
    if [[ $NIGHTLY ]]; then
      echo "set DEMISTO_SDK_GRAPH_FORCE_CREATE to true to create graph from scratch"
      export DEMISTO_SDK_GRAPH_FORCE_CREATE=true
      echo "DEMISTO_SDK_GRAPH_FORCE_CREATE was set to true to create graph from scratch"
      echo $DEMISTO_SDK_GRAPH_FORCE_CREATE
    fi
  - section_end "Source BASH Environment"
  - section_start "Granting execute permissions on files" --collapsed
  - chmod +x ./Tests/scripts/*
  - chmod +x ./Tests/Marketplace/*
  - section_end "Granting execute permissions on files"

.get_contribution_pack: &get_contribution_pack
  - section_start "getting contrib packs" --collapsed
  - |
    if [[ -n "${CONTRIB_BRANCH}" ]]; then
      USERNAME=$(echo $CONTRIB_BRANCH | cut -d ":" -f 1)
      BRANCH=$(echo $CONTRIB_BRANCH | cut -d ":" -f 2)
      python3 ./Utils/update_contribution_pack_in_base_branch.py -p $PULL_REQUEST_NUMBER -b $BRANCH -u $USERNAME -c $CONTRIB_REPO
    fi
  - section_end "getting contrib packs"

.install_venv: &install_venv
  - section_start "Installing Virtualenv" --collapsed
  # we still need to install even if cached. if cached, `poetry` will handle it
  - echo "installing venv, with always copy:${POETRY_VIRTUALENVS_OPTIONS_ALWAYS_COPY}"
  - NO_HOOKS=1 .hooks/bootstrap | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
  - echo "Checking if pyproject.toml is consistent with poetry.lock"
  - poetry lock --check
  - npm ci --cache .npm --prefer-offline | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
  - source ./.venv/bin/activate
  - |
    if [[ -n "${DEMISTO_SDK_NIGHTLY}" || -n "${OVERRIDE_SDK_REF}" ]]; then
      echo "Installing SDK from ${SDK_REF}" | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
      pip3 uninstall -y demisto-sdk | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
      pip3 install "git+https://github.com/demisto/demisto-sdk@${SDK_REF}#egg=demisto-sdk" | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
    else
      echo "Using SDK from pyproject.toml" | tee --append "${ARTIFACTS_FOLDER}/logs/installations.log"
    fi
  - |
      python3 --version | tee -a "${ARTIFACTS_FOLDER}/logs/installed_python_libraries.log"
      python3 -m pip list | tee -a "${ARTIFACTS_FOLDER}/logs/installed_python_libraries.log"
  - section_end "Installing Virtualenv"

.ssh-config-setup:
  - section_start "SSH config setup" --collapsed
  - cp $GCP_SSH_CONFIGURATION ~/.ssh/config
  - chmod 700 ~/.ssh/config
  - section_end "SSH config setup"

.install_ssh_keys: &install_ssh_keys
  - section_start "Installing SSH keys" --collapsed
  - eval $(ssh-agent -s)
  - chmod 400 $OREGON_CI_KEY
  - ssh-add $OREGON_CI_KEY
  - mkdir -p ~/.ssh
  - chmod 700 ~/.ssh
  - section_end "Installing SSH keys"

.install_node_modules: &install_node_modules
  - section_start "Installing node modules" --collapsed
  - source $NVM_DIR/nvm.sh
  - nvm use default | tee --append "${ARTIFACTS_FOLDER}/logs/installations_node.log"
  - echo "Installing Node Modules" | tee --append "${ARTIFACTS_FOLDER}/logs/installations_node.log"
  - npm ci --cache .npm --prefer-offline >> "${ARTIFACTS_FOLDER}/logs/installations_node.log" 2>&1
  - npm list --json >> "${ARTIFACTS_FOLDER}/logs/installations_node.log" 2>&1
  - npm link jsdoc-to-markdown@5.0.3 | tee  >> "${ARTIFACTS_FOLDER}/logs/installations_node.log" 2>&1  # disable-secrets-detection
  - section_end "Installing node modules"

.get_last_upload_commit: &get_last_upload_commit
  - section_start "Getting last bucket upload commit" --collapsed
  - gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
  - gsutil cp "gs://$GCS_PRODUCTION_BUCKET/content/packs/index.json" "${ARTIFACTS_FOLDER_SERVER_TYPE}/previous_index.json"
  - export LAST_UPLOAD_COMMIT=$(cat "${ARTIFACTS_FOLDER_SERVER_TYPE}/previous_index.json" | jq -r ".\"commit\"")
  - section_end "Getting last bucket upload commit"

.neo4j-setup: &neo4j-setup
  - section_start "Neo4j Setup" --collapsed
  - neo4j-admin dbms set-initial-password contentgraph
  - neo4j start
  - section_end "Neo4j Setup"

.build_parameters: &build_parameters
  - section_start "Build Parameters" --collapsed
  - echo "Environment Variables:"
  - set | grep -E "^ARTIFACTS_FOLDER.*=|^JIRA_.*=|^NIGHTLY=|^INSTANCE_TESTS=|^SERVER_BRANCH_NAME=|^ARTIFACT_BUILD_NUM=|^DEMISTO_SDK_NIGHTLY=|^TIME_TO_LIVE=|^CONTRIB_BRANCH=|^FORCE_PACK_UPLOAD=|^PACKS_TO_UPLOAD=|^BUCKET_UPLOAD=|^STORAGE_BASE_PATH=|^OVERRIDE_ALL_PACKS=|^GCS_MARKET_BUCKET=|^GCS_MARKET_V2_BUCKET=|^GCS_MARKET_XPANSE_BUCKET=|^SLACK_.*=|^NVM_DIR=|^NODE_VERSION=|^PATH=|^ARTIFACTS_FOLDER=|^ARTIFACTS_FOLDER_INSTANCE=|^ARTIFACTS_FOLDER_SERVER_TYPE=|^ENV_RESULTS_PATH=|^LAST_UPLOAD_COMMIT=|^DEMISTO_SDK_LOG_FILE_SIZE=|^DEMISTO_SDK_LOG_FILE_COUNT=|^DEMISTO_SDK_LOG_FILE_PATH=|^DEMISTO_SDK_LOG_NO_COLORS=|^DEMISTO_SDK_LOG_NOTIFY_PATH=|^POETRY_VIRTUALENVS_OPTIONS_ALWAYS_COPY=|^DEMISTO_SDK_NIGHTLY=|^OVERRIDE_SDK_REF=|^SDK_REF=" | sort
  - echo "Versions Installed:"
  - python --version
  - python3 --version
  - poetry --version
  - pip3 --version
  - node --version
  - npm --version
  - jsdoc2md --version
  - demisto-sdk --version
  - section_end "Build Parameters"

.gitlab_ci_build_parameters: &gitlab_ci_build_parameters
  - section_start "Gitlab CI Build Parameters" --collapsed
  - set | grep -E "^CI_.*=|^GITLAB.*=" | sort
  - section_end "Gitlab CI Build Parameters"

.checkout-upload-commit-content-nightly: &checkout-upload-commit-content-nightly
  - section_start "Checkout upload commit content nightly" --collapsed
  - | 
    if [[ -n "${NIGHTLY}" && "${CI_COMMIT_BRANCH}" == "master" ]]; then
      if [[ ! -d "${CI_PROJECT_DIR}/artifacts/production_packs" ]]; then
        echo "content production packs do not exist in ${CI_PROJECT_DIR}/artifacts"
        exit 1
      fi
      rm -rf ./Packs
      echo "copying production Packs folder from ${CI_PROJECT_DIR}/artifacts/production_packs to ./Packs"
      cp -r ${CI_PROJECT_DIR}/artifacts/production_packs ./Packs
      git config core.fileMode false  # used to tell git not to identify permission changes on files as changes
      chmod -R 777 ./Packs  # required for the lint, we use the permissions of the file when running pytest within the containers
      echo "the Packs changes between upload commit ${LAST_UPLOAD_COMMIT} to master commit ${CI_COMMIT_SHA} is:"
      git status -- Packs  # show the differences between the upload commit to the master branch for the Packs folder
      echo "The Packs folder is in the state of commit $LAST_UPLOAD_COMMIT"
    else
      echo "not checking out to the latest upload commit $LAST_UPLOAD_COMMIT because the build is not content nightly"
    fi
  - |
  - section_end "Checkout upload commit content nightly"

.export_cloud_machine_constants: &export_cloud_machine_constants
  # exporting the machine credentials
  - CLOUD_SERVERS_PATH=$(cat $CLOUD_SERVERS_FILE)
  - cat "${CLOUD_API_KEYS}" > "cloud_api_keys.json"
  - cat "${CLOUD_API_TOKENS}" > "cloud_api_tokens.json"

  - IFS=', ' read -r -a CLOUD_CHOSEN_MACHINE_ID_ARRAY <<< "${CLOUD_CHOSEN_MACHINE_IDS}"
  - |
    for CLOUD_CHOSEN_MACHINE_ID in "${CLOUD_CHOSEN_MACHINE_ID_ARRAY[@]}"; do
      export XSIAM_SERVER_CONFIG=$(jq -r ".[\"${CLOUD_CHOSEN_MACHINE_ID}\"]" < "$CLOUD_SERVERS_PATH")
      export DEMISTO_BASE_URL=$(echo "$XSIAM_SERVER_CONFIG" | jq -r ".[\"base_url\"]")
      export XSIAM_AUTH_ID=$(echo "$XSIAM_SERVER_CONFIG" | jq -r ".[\"x-xdr-auth-id\"]")
      export DEMISTO_API_KEY=$(jq -r ".[\"${CLOUD_CHOSEN_MACHINE_ID}\"]" < "cloud_api_keys.json")
      export XSIAM_TOKEN=$(jq -r ".[\"${CLOUD_CHOSEN_MACHINE_ID}\"]" < "cloud_api_tokens.json")
      break
    done

.default-before-script:
  before_script:
    - source .gitlab/helper_functions.sh
    - *setup-network-certs
    - *setup-artifactory
    - *stop_contrib_external_build
    - *create_artifacts_and_server_type_instance_folders
    - *clone_and_export_variables
    - *check_build_files_are_up_to_date
    - *install_node_modules
    - *install_venv
    - *get_contribution_pack
    - *get_last_upload_commit
    - *install_ssh_keys
    - *neo4j-setup
    - *build_parameters
    - *gitlab_ci_build_parameters
    - *checkout-upload-commit-content-nightly

.default-after-script:
  - source .gitlab/helper_functions.sh
  - *setup-network-certs
  - *setup-artifactory
  - *install_node_modules
  - *install_venv

.add-content-production-to-artifacts:
  - section_start "Clone production content and add it to artifacts" --collapsed
  - mkdir content_production
  - cd content_production
  - git init > /dev/null 2>&1
  - git remote add origin https://gitlab-ci-token:${CI_JOB_TOKEN}@${CI_SERVER_HOST}/${CI_PROJECT_NAMESPACE}/content.git
  - git fetch --depth 1 origin $LAST_UPLOAD_COMMIT
  - git checkout FETCH_HEAD >${ARTIFACTS_FOLDER}/logs/add-content-production-to-artifacts.log 2>&1
  - cp -r ./Packs ${ARTIFACTS_FOLDER}/production_packs
  - echo "checked out ${LAST_UPLOAD_COMMIT} which is the last successful upload commit"
  - section_end "Clone production content and add it to artifacts"
  - job-done


.default-job-settings:
  interruptible: true
  extends:
    - .default-cache
    - .default-before-script
  needs:
    - job: cloning-content-repo-last-upload-commit
      optional: true

.trigger-slack-notification:
  stage: .post
  trigger:
    strategy: depend
    include:
      - local: .gitlab/ci/.gitlab-ci.slack-notify.yml
  inherit: # see https://gitlab.com/gitlab-org/gitlab-runner/-/issues/27775
    variables: false

.destroy_xsoar_instances:
  - section_start "Destroy Instances"
  - python3 ./Tests/scripts/destroy_instances.py --artifacts-dir "${ARTIFACTS_FOLDER}" --env-file "${ENV_RESULTS_PATH}" --instance-role "${INSTANCE_ROLE}"
  - destroy_instances_exit_code=$?
  - |
    if [ "${destroy_instances_exit_code}" -ne 0 ]; then
      echo "Failed to destroy instances of role ${INSTANCE_ROLE}, exit code: ${destroy_instances_exit_code}"
    fi
  - section_end "Destroy Instances"

.lock-machine:
  - section_start "Lock Machine" --collapsed
  - echo "Authenticating GCP"
  - gcloud auth activate-service-account --key-file="$GCS_ARTIFACTS_KEY" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
  - echo "Auth done successfully"
  - ./Tests/scripts/wait_in_line_for_cloud_env.sh "$CLOUD_MACHINES_TYPE"
  - source CloudEnvVariables
  - echo "CLOUD Chosen machine ids are:${CLOUD_CHOSEN_MACHINE_IDS}" | tee "${ARTIFACTS_FOLDER}/logs/lock_file.txt"
  - section_end "Lock Machine"

.unlock-machine:
  - section_start "Unlock Machine" --collapsed
  - |
    if [[ -f "CloudEnvVariables" ]]; then
      source CloudEnvVariables

      if [[ -n "${CLOUD_CHOSEN_MACHINE_IDS}" ]]; then
        if [[ -e "${ARTIFACTS_FOLDER}/logs/lock_file.txt" ]]; then
          echo "Job finished, removing lock file for machine ids:${CLOUD_CHOSEN_MACHINE_IDS}"
          gcloud auth activate-service-account --key-file="$GCS_ARTIFACTS_KEY" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
          gsutil rm "gs://xsoar-ci-artifacts/$GCS_LOCKS_PATH/machines_locks/*-lock-$CI_JOB_ID"
          echo "Finished removing lock file(s)"
        else
          echo "No lock file found, skipping unlocking"
        fi
      else
        echo "No machine ids were chosen, skipping unlocking"
      fi
    else
      echo "No CloudEnvVariables file found, skipping unlocking"
    fi
  - section_end "Unlock Machine"

.cloud-machine-information:
  - section_start "Cloud Machine information"
  - ./Tests/scripts/print_cloud_machine_details.sh
  - section_end "Cloud Machine information"

.uninstall-packs-and-reset-bucket-cloud:
  - section_start "Uninstall Packs and Reset Bucket Cloud" --collapsed
  - ./Tests/scripts/uninstall_packs_and_reset_bucket_cloud.sh || EXIT_CODE=$?
  - section_end "Uninstall Packs and Reset Bucket Cloud"


.validate_content_test_conf_branch_merged:
  - section_start "Validate content-test-conf Branch Merged"
  - |
    if [[ "${CI_COMMIT_BRANCH}" = "master" ]]; then
      echo "Skipping, Should not run on master branch."
    elif [ 'true' = $(./Tests/scripts/check_if_branch_exist.sh -u "gitlab-ci-token" -t "${CI_JOB_TOKEN}" -h "${CI_SERVER_HOST}" --repo "${CI_PROJECT_NAMESPACE}/content-test-conf" -b "${CI_COMMIT_BRANCH}") ]; then
      RED='\033[0;31m'
      NC='\033[0m'
      echo -e "${RED}ERROR: Found a branch with the same name:${CI_COMMIT_BRANCH} in contest-test-conf repository.\n Merge it in order to merge the current branch into content repo.${NC}"
      job-done
      exit 1
    else
      echo "Couldn't find a branch with the name:${CI_COMMIT_BRANCH} in contest-test-conf repository."
    fi
  - section_end "Validate content-test-conf Branch Merged"

.unittests-and-lint-settings:
  tags:
    - gce
  stage: unittests-and-validations
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: ${CI_PROJECT_DIR}/artifacts/coverage_report/coverage.xml
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/unit-tests
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  services:
    - name: ${DOCKER_IO}/library/docker:20.10.12-dind
      alias: docker
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: ""
  extends:
    - .default-job-settings


.run-unittests-and-lint:
  extends:
    - .unittests-and-lint-settings
  script:
    - section_start "Test Infrastructure"
    - python3 -m pytest ./Tests/scripts/infrastructure_tests/ -v --disable-warnings
    - python3 -m pytest ./Tests/Marketplace/Tests/ -v --disable-warnings
    - python3 -m pytest ./Tests/tests -v --disable-warnings
    - python3 -m pytest ./Tests/private_build/ -v --disable-warnings
    - python3 -m pytest Utils -v --disable-warnings
    - |
      if [ -n "${DEMISTO_SDK_NIGHTLY}" ]; then
        ./Tests/scripts/sdk_pylint_check.sh
      fi
    - section_end "Test Infrastructure"
    - section_start "Revoking GCP Auth and Configure Docker"
    - gcloud auth revoke "${GCS_ARTIFACTS_ACCOUNT_NAME}" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
    - gcloud auth configure-docker ${DOCKER_IO_DOMAIN} >> "${ARTIFACTS_FOLDER}/logs/configure_docker_with_registry.log" 2>&1
    - section_end "Revoking GCP Auth and Configure Docker"
    - section_start "Run Unit Testing and Lint"
    - |
      if [[ -n $BUCKET_UPLOAD && $TEST_UPLOAD == "true" ]]; then
        echo "Skipping validations when uploading to a test bucket."
      else
        echo "demisto-sdk version: $(demisto-sdk --version)"
        echo "mypy version: $(mypy --version)"
        echo "flake8 py3 version: $(python3 -m flake8 --version)"
        echo "bandit py3 version: $(python3 -m bandit --version 2>&1)"
        echo "vulture py3 version: $(python3 -m vulture --version 2>&1)"
        set -ex
        SHOULD_LINT_ALL=$(./Tests/scripts/should_lint_all.sh)
        echo "here before mkdir"
        pwd
        ls -lh
        mkdir ./unit-tests
        echo "here after mkdir"
        if [[ -n "${SHOULD_LINT_ALL}" || -n "${BUCKET_UPLOAD}" ]]; then
          echo "In should lint all / bucket upload."
          if [ $DOCKER == "native:dev,from-yml" ]; then
            echo "removing native:dev from docker list."
            DOCKER='from-yml'
          fi
        fi
        if [ -n "$SHOULD_LINT_ALL" ]; then
          echo -e  "----------\nLinting all because:\n${SHOULD_LINT_ALL}\n----------"
          demisto-sdk lint -p 10 -a --test-xml ./unit-tests --log-path "${ARTIFACTS_FOLDER}" --failure-report "${ARTIFACTS_FOLDER}" --coverage-report "${ARTIFACTS_FOLDER}/coverage_report" -dt 120 --time-measurements-dir "${ARTIFACTS_FOLDER}" --docker-image ${DOCKER}
        fi
        if [[ -n $BUCKET_UPLOAD ]]; then
          demisto-sdk lint --console-log-threshold DEBUG -p 8 -g --no-mypy --prev-ver $LAST_UPLOAD_COMMIT -v --test-xml ./unit-tests --log-path "${ARTIFACTS_FOLDER}" --failure-report "${ARTIFACTS_FOLDER}" --coverage-report "${ARTIFACTS_FOLDER}/coverage_report" --check-dependent-api-module --docker-image ${DOCKER}
        else
          if [[ "$CI_COMMIT_BRANCH" != "master" ]]; then
            # skip running `lint -g` on master
            demisto-sdk lint --console-log-threshold DEBUG -p 8 -g --test-xml ./unit-tests --log-path ./artifacts --failure-report ./artifacts --coverage-report "${ARTIFACTS_FOLDER}/coverage_report" --check-dependent-api-module --docker-image ${DOCKER}
          fi
        fi
      
        if [[ -f "${ARTIFACTS_FOLDER}/coverage_report/.coverage" ]]; then
          if [[ "$CI_PIPELINE_SOURCE" == "schedule" ||   -n "$SHOULD_LINT_ALL"  ||  -n "${NIGHTLY}" || -n "${BUCKET_UPLOAD}" || -n "${DEMISTO_SDK_NIGHTLY}" ]]; then
            demisto-sdk coverage-analyze -i "${ARTIFACTS_FOLDER}/coverage_report/.coverage" --report-dir "${ARTIFACTS_FOLDER}/coverage_report" --report-type all --allowed-coverage-degradation-percentage 100
            if [[ -n "${NIGHTLY}" && "$CI_COMMIT_BRANCH" == "master" && $DOCKER == "from-yml" ]]; then
              python3 Utils/upload_code_coverage_report.py --service_account $GCS_MARKET_KEY --source_file_name "${ARTIFACTS_FOLDER}/coverage_report/coverage.json" --minimal_file_name "${ARTIFACTS_FOLDER}/coverage_report/coverage-min.json"
            fi
          else
            demisto-sdk coverage-analyze -i "${ARTIFACTS_FOLDER}/coverage_report/.coverage" --report-dir "${ARTIFACTS_FOLDER}/coverage_report" --report-type html,xml --previous-coverage-report-url https://storage.googleapis.com/marketplace-dist-dev/code-coverage-reports/coverage-min.json
          fi
        fi
      fi
    - section_end "Run Unit Testing and Lint"
    - job-done
  parallel:
    matrix:
      - DOCKER: ['native:ga,native:maintenance,native:candidate','native:dev,from-yml']

.run-validations:
  stage: unittests-and-validations
  extends:
    - .default-job-settings
  artifacts:
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  script:
    - section_start "Look For Secrets"
    - demisto-sdk secrets --post-commit --ignore-entropy
    - section_end "Look For Secrets"
    - section_start "Copy conf.json To Server Type Artifacts Folder"
    - cp "./Tests/conf.json" "${ARTIFACTS_FOLDER_SERVER_TYPE}/conf.json"
    - section_end "Copy conf.json To Server Type Artifacts Folder"
    - section_start "Validate Files and Yaml"
    - |
      ./Tests/scripts/linters_runner.sh
      ./Tests/scripts/validate.sh
    - section_end "Validate Files and Yaml"
    - section_start "Check Spelling"
    - python3 ./Tests/scripts/circleci_spell_checker.py $CI_COMMIT_BRANCH
    - section_end "Check Spelling"
    - section_start "Validate landingPageSections.json"
    - echo "Download index.zip"
    - INDEX_PATH=$(mktemp)
    - |
      gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
      echo "successfully activated google cloud service account"
      gsutil cp "gs://marketplace-dist/content/packs/index.zip" $INDEX_PATH
      echo "successfully downloaded index.zip"
    - echo "successfully downloaded index.zip into $INDEX_PATH"

    - UNZIP_PATH=$(mktemp -d)
    - unzip $INDEX_PATH -d $UNZIP_PATH > "${ARTIFACTS_FOLDER}/logs/unzip_index.log"

    - python3 Tests/Marketplace/validate_landing_page_sections.py -i $UNZIP_PATH
    - section_end "Validate landingPageSections.json"
    - section_start "Revoking GCP Auth"
    - gcloud auth revoke "${GCS_ARTIFACTS_ACCOUNT_NAME}" >> "${ARTIFACTS_FOLDER}/logs/gcloud_auth.log" 2>&1
    - section_end "Revoking GCP Auth"
    - !reference [ .validate_content_test_conf_branch_merged ]  # This section should be the last one in the script, do not move it.
    - job-done

.run-validations-new-validate-flow:
  stage: unittests-and-validations
  extends:
    - .default-job-settings
  artifacts:
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  script:
    - section_start "Validate Files and Yaml"
    - |
      ./Tests/scripts/linters_runner.sh
      ./Tests/scripts/new_validate.sh
    - section_end "Validate Files and Yaml"
    - !reference [ .validate_content_test_conf_branch_merged ]  # This section should be the last one in the script, do not move it.
    - job-done

.jobs-done-check:
  stage: are-jobs-really-done
  extends:
    - .default-job-settings
  script:
    - python3 Tests/scripts/check_jobs_done.py --triggering-workflow "${WORKFLOW}" --job-done-files "${PIPELINE_JOBS_FOLDER}"

cloning-content-repo-last-upload-commit:
  stage: .pre
  rules:
    - if: '$NIGHTLY'
  artifacts:
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  before_script:
    - source .gitlab/helper_functions.sh
    - *setup-network-certs
    - *create_artifacts_and_server_type_instance_folders
    - *get_last_upload_commit
  variables:
    ARTIFACTS_FOLDER: ${CI_PROJECT_DIR}/artifacts
  script:
    - !reference [.add-content-production-to-artifacts]
