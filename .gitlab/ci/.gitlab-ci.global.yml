.default-cache:
  cache:
    key:
      files:
        - "poetry.lock"
        - "package-lock.json"
      prefix: dev-content
    paths:
      - $PIP_CACHE_DIR
      - .venv/
      - node_modules/
      - .npm/
    policy: pull


.setup-network-certs: &setup-network-certs
  - section_start "Setup network certs" --collapsed
  - chmod 700 ${CERTIFICATE_SETUP_SCRIPT}
  - source ${CERTIFICATE_SETUP_SCRIPT}
  - section_end "Setup network certs"

.setup-artifactory: &setup-artifactory
  - section_start "Setup Artifactory" --collapsed
  - chmod 700 ${ARTIFACTORY_SETUP_SCRIPT}
  - source ${ARTIFACTORY_SETUP_SCRIPT}
  - section_end "Setup Artifactory"

### Global Script Snippets ###

.create-id-set:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - section_end "Create ID Set"

.create-id-set-xsoar:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xsoar" >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - section_end "Create ID Set"

.create-id-set-mp-v2:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "marketplacev2" >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - section_end "Create ID Set"

.create-id-set-xpanse:
  - section_start "Create ID Set" --collapsed
  - demisto-sdk create-id-set -o ./Tests/id_set.json --marketplace "xpanse" >> $ARTIFACTS_FOLDER/logs/create_id_set.log
  - cp ./Tests/id_set.json $ARTIFACTS_FOLDER
  - if [ -f ./all_removed_items_from_id_set.json ]; then cp ./all_removed_items_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - if [ -f ./items_removed_manually_from_id_set.json ]; then cp ./items_removed_manually_from_id_set.json $ARTIFACTS_FOLDER/logs; fi
  - section_end "Create ID Set"

.download-demisto-conf:
  - section_start "Download content-test-conf and infra" --collapsed
  - ./Tests/scripts/download_conf_repos.sh 2>&1 | tee --append "${ARTIFACTS_FOLDER}/logs/download_conf_repos.log"
  - section_end "Download content-test-conf and infra"

.secrets-fetch:
  - section_start "Secrets Fetch" --collapsed
  - SECRET_CONF_PATH=$(cat secret_conf_path)
  - python3 ./Tests/scripts/add_secrets_file_to_build.py -sa "$GSM_SERVICE_ACCOUNT" -sf "$SECRET_CONF_PATH" -u "$DEMISTO_USERNAME" -p "$DEMISTO_PASSWORD" --gsm_project_id_dev "$GSM_PROJECT_ID_DEV" --gsm_project_id_prod "$GSM_PROJECT_ID" >> $ARTIFACTS_FOLDER/logs/handle_secrets.log
  - section_end "Secrets Fetch"

.check_build_files_are_up_to_date: &check_build_files_are_up_to_date
  - section_start "Check Build Files Are Up To Date"
  - |
    if [[ -n "${DEMISTO_SDK_NIGHTLY}" || -n "${NIGHTLY}" || -n "${BUCKET_UPLOAD}" || -n "${SLACK_JOB}" ]]; then
      echo "running nightly or bucket upload, skipping files up-to-date validation"
    else
      ./Tests/scripts/is_file_up_to_date.sh .gitlab $CI_COMMIT_BRANCH
      # we want to checkout if it's not up-to-date
      ./Tests/scripts/is_file_up_to_date.sh poetry.lock $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh pyproject.toml $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_mpv2_list.json $CI_COMMIT_BRANCH true
      ./Tests/scripts/is_file_up_to_date.sh Tests/Marketplace/core_packs_xpanse_list.json $CI_COMMIT_BRANCH true
    fi
  - section_end "Check Build Files Are Up To Date"

.cleanup_logs_directory: &cleanup_logs_directory
  - section_start "Creating new clean logs directory" --collapsed
  - rm -rf $ARTIFACTS_FOLDER/logs
  - mkdir -p $ARTIFACTS_FOLDER/logs
  - section_end "Creating new clean logs directory"

.stop_contrib_external_build: &stop_contrib_external_build
  - |
    if [[ $CI_COMMIT_BRANCH =~ ^contrib/* && $CI_PIPELINE_SOURCE = "push" && $(curl --header "Accept: application/vnd.github.v3.raw" --header "Authorization: token $GITHUB_TOKEN" "https://api.github.com/repos/demisto/content/pulls?state=open&base=master&head=demisto:${CI_COMMIT_BRANCH}" | jq) = '[]' ]]; then
      echo "not running on contrib/ branches when there is no open internal PR or the pipeline was manually triggered"
      set -e
      exit 1
    fi

.clone_and_export_variables: &clone_and_export_variables
  - section_start "Git - Job Start Actions" --collapsed
  - git checkout -B $CI_COMMIT_BRANCH $CI_COMMIT_SHA
  - git config diff.renameLimit 6000
  - section_end "Git - Job Start Actions"
  - section_start "Create artifacts directory" --collapsed
  - mkdir -p -m 777 $ARTIFACTS_FOLDER/
  - section_end "Create artifacts directory"
  - section_start "Source BASH Environment" --collapsed
  - |
    if [[ -f "$BASH_ENV" ]]; then
      source "$BASH_ENV"
    fi
  - source .circleci/content_release_vars.sh
  - section_end "Source BASH Environment"
  - section_start "Granting execute permissions on files" --collapsed
  - chmod +x ./Tests/scripts/*
  - chmod +x ./Tests/Marketplace/*
  - section_end "Granting execute permissions on files"

.get_contribution_pack: &get_contribution_pack
  - section_start "getting contrib packs" --collapsed
  - |
    if [[ -n "${CONTRIB_BRANCH}" ]]; then
      REPO=$(echo $CONTRIB_BRANCH | cut -d ":" -f 1)
      BRANCH=$(echo $CONTRIB_BRANCH | cut -d ":" -f 2)
      python3 ./Utils/update_contribution_pack_in_base_branch.py -p $PULL_REQUEST_NUMBER -b $BRANCH -c $REPO
    fi
  - section_end "getting contrib packs"

.install_venv: &install_venv
  - section_start "Installing Virtualenv" --collapsed
  # we still need to install even if cached. if cached, `poetry` will handle it
  - echo "installing venv, with always copy:${POETRY_VIRTUALENVS_OPTIONS_ALWAYS_COPY}"
  - NO_HOOKS=1 .hooks/bootstrap | tee --append $ARTIFACTS_FOLDER/logs/installations.log
  - echo "Checking if pyproject.toml is consistent with poetry.lock"
  - poetry lock --check
  - npm ci --cache .npm --prefer-offline | tee --append $ARTIFACTS_FOLDER/logs/installations.log
  - source ./.venv/bin/activate
  - |
    if [[ -n "${DEMISTO_SDK_NIGHTLY}" || -n "${OVERRIDE_SDK_REF}" ]]; then
      echo "Installing SDK from $SDK_REF" | tee --append $ARTIFACTS_FOLDER/logs/installations.log
      pip3 uninstall -y demisto-sdk | tee --append $ARTIFACTS_FOLDER/logs/installations.log
      pip3 install "git+https://github.com/demisto/demisto-sdk@${SDK_REF}#egg=demisto-sdk" | tee --append $ARTIFACTS_FOLDER/logs/installations.log
    fi
  - |
      python3 --version | tee -a $ARTIFACTS_FOLDER/installed_python_libraries.txt
      python3 -m pip list | tee -a $ARTIFACTS_FOLDER/installed_python_libraries.txt
  - section_end "Installing Virtualenv"

.ssh-config-setup:
  - section_start "SSH config setup" --collapsed
  - cp $GCP_SSH_CONFIGURATION ~/.ssh/config
  - chmod 700 ~/.ssh/config
  - section_end "SSH config setup"

.install_ssh_keys: &install_ssh_keys
  - section_start "Installing SSH keys" --collapsed
  - eval $(ssh-agent -s)
  - chmod 400 $OREGON_CI_KEY
  - ssh-add $OREGON_CI_KEY
  - mkdir -p ~/.ssh
  - chmod 700 ~/.ssh
  - section_end "Installing SSH keys"

.install_node_modules: &install_node_modules
  - section_start "Installing node modules" --collapsed
  - source $NVM_DIR/nvm.sh
  - nvm use default | tee --append $ARTIFACTS_FOLDER/logs/installations_node.log
  - echo "Installing Node Modules" | tee --append $ARTIFACTS_FOLDER/logs/installations_node.log
  - npm ci --cache .npm --prefer-offline | tee --append $ARTIFACTS_FOLDER/logs/installations_node.log
  - npm list --json | tee --append $ARTIFACTS_FOLDER/logs/installations_node.log
  - npm link jsdoc-to-markdown@5.0.3 | tee --append $ARTIFACTS_FOLDER/logs/installations_node.log  # disable-secrets-detection
  - section_end "Installing node modules"

.get_last_upload_commit: &get_last_upload_commit
  - section_start "Getting last bucket upload commit"
  - gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" > $ARTIFACTS_FOLDER/logs/gauth.out 2>$ARTIFACTS_FOLDER/logs/gauth.err
  - gsutil cp "gs://$GCS_MARKET_BUCKET/content/packs/index.json" "$ARTIFACTS_FOLDER/previous_index.json"
  - export LAST_UPLOAD_COMMIT=$(cat $ARTIFACTS_FOLDER/previous_index.json | jq -r ".\"commit\"")
  - section_end "Getting last bucket upload commit"

.neo4j-setup: &neo4j-setup
  - section_start "Neo4j Setup"
  - neo4j-admin dbms set-initial-password contentgraph
  - neo4j start
  - section_end "Neo4j Setup"

.build_parameters: &build_parameters
  - section_start "Build Parameters"
  - set | grep -E "^NIGHTLY=|^INSTANCE_TESTS=|^SERVER_BRANCH_NAME=|^ARTIFACT_BUILD_NUM=|^DEMISTO_SDK_NIGHTLY=|^TIME_TO_LIVE=|^CONTRIB_BRANCH=|^FORCE_PACK_UPLOAD=|^PACKS_TO_UPLOAD=|^BUCKET_UPLOAD=|^STORAGE_BASE_PATH=|^OVERRIDE_ALL_PACKS=|^GCS_MARKET_BUCKET=|^GCS_MARKET_V2_BUCKET=|^GCS_MARKET_XPANSE_BUCKET=|^SLACK_CHANNEL=|^NVM_DIR=|^NODE_VERSION=|^PATH=|^ARTIFACTS_FOLDER=|^ENV_RESULTS_PATH=|^LAST_UPLOAD_COMMIT=|^DEMISTO_SDK_LOG_FILE_SIZE=|^DEMISTO_SDK_LOG_FILE_COUNT=|^DEMISTO_SDK_LOG_FILE_PATH=|^DEMISTO_SDK_LOG_NO_COLORS=|^DEMISTO_SDK_LOG_NOTIFY_PATH=|^POETRY_VIRTUALENVS_OPTIONS_ALWAYS_COPY="
  - python --version
  - python3 --version
  - poetry --version
  - pip3 --version
  - node --version
  - npm --version
  - jsdoc2md --version
  - demisto-sdk --version
  - section_end "Build Parameters"


.default-before-script:
  before_script:
    - source .gitlab/helper_functions.sh
    - *setup-network-certs
    - *setup-artifactory
    - *stop_contrib_external_build
    - *clone_and_export_variables
    - *check_build_files_are_up_to_date
    - *cleanup_logs_directory
    - *install_node_modules
    - *install_venv
    - *get_contribution_pack
    - *get_last_upload_commit
    - *install_ssh_keys
    - *neo4j-setup
    - *build_parameters

.default-after-script:
  - source .gitlab/helper_functions.sh
  - *setup-network-certs
  - *setup-artifactory
  - *install_node_modules
  - *install_venv

.default-job-settings:
  interruptible: true
  extends:
    - .default-cache
    - .default-before-script

.trigger-slack-notification:
  stage: .post
  trigger:
    strategy: depend
    include:
      - local: .gitlab/ci/.gitlab-ci.slack-notify.yml
  inherit: # see https://gitlab.com/gitlab-org/gitlab-runner/-/issues/27775
    variables: false

.lock-machine:
  - section_start "Lock Machine" --collapsed
  - echo "Authenticating GCP"
  - gcloud auth activate-service-account --key-file="$GCS_ARTIFACTS_KEY" > $ARTIFACTS_FOLDER/logs/gauth.out 2>$ARTIFACTS_FOLDER/logs/gauth.err
  - echo "Auth done successfully"
  - ./Tests/scripts/wait_in_line_for_cloud_env.sh "$CLOUD_MACHINES_TYPE"
  - source CloudEnvVariables
  - echo "CLOUD Chosen machine ids are:${CLOUD_CHOSEN_MACHINE_IDS}" | tee $ARTIFACTS_FOLDER/logs/lock_file.txt
  - section_end "Lock Machine"

.unlock-machine:
  - section_start "Unlock Machine" --collapsed
  - source CloudEnvVariables
  - |
    if [[ -n "${CLOUD_CHOSEN_MACHINE_IDS}" ]]; then
      if [[ -e "$ARTIFACTS_FOLDER/logs/lock_file.txt" ]]; then
        echo "Job finished, removing lock file for machine ids:${CLOUD_CHOSEN_MACHINE_IDS}"
        gcloud auth activate-service-account --key-file="$GCS_ARTIFACTS_KEY" > $ARTIFACTS_FOLDER/logs/gauth.out 2>$ARTIFACTS_FOLDER/logs/gauth.err
        gsutil rm "gs://xsoar-ci-artifacts/$GCS_LOCKS_PATH/machines_locks/*-lock-$CI_JOB_ID"
        echo "Finished removing lock file"
      else
        echo "No lock file found, skipping unlocking"
      fi
    else
      echo "No machine ids were chosen, skipping unlocking"
    fi
  - section_end "Unlock Machine"

.cloud-machine-information:
  - section_start "Cloud Machine information"
  - ./Tests/scripts/print_cloud_machine_details.sh
  - section_end "Cloud Machine information"

.uninstall-packs-and-reset-bucket-cloud:
  - section_start "Uninstall Packs and Reset Bucket Cloud" --collapsed
  - ./Tests/scripts/uninstall_packs_and_reset_bucket_cloud.sh || EXIT_CODE=$?
  - section_end "Uninstall Packs and Reset Bucket Cloud"


.validate_content_test_conf_branch_merged:
  - section_start "Validate content-test-conf Branch Merged"
  - |
    if [[ "${CI_COMMIT_BRANCH}" = "master" ]]; then
      echo "Skipping, Should not run on master branch."
    elif [ 'true' = $(./Tests/scripts/check_if_branch_exist.sh -u "gitlab-ci-token" -t "${CI_JOB_TOKEN}" -h "${CI_SERVER_HOST}" --repo "${CI_PROJECT_NAMESPACE}/content-test-conf" -b "${CI_COMMIT_BRANCH}") ]; then
      RED='\033[0;31m'
      NC='\033[0m'
      echo -e "${RED}ERROR: Found a branch with the same name:${CI_COMMIT_BRANCH} in contest-test-conf repository.\n Merge it in order to merge the current branch into content repo.${NC}"
      job-done
      exit 1
    else
      echo "Couldn't find a branch with the name:${CI_COMMIT_BRANCH} in contest-test-conf repository."
    fi
  - section_end "Validate content-test-conf Branch Merged"

.unittests-and-lint-settings:
  tags:
    - gce
  needs: [ ]
  stage: unittests-and-validations
  artifacts:
    reports:
      coverage_report:
        coverage_format: cobertura
        path: ${CI_PROJECT_DIR}/artifacts/coverage_report/coverage.xml
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/unit-tests
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  services:
    - name: docker.art.code.pan.run/build-tools--image-dind:20.10.12-dind
      alias: docker
  variables:
    DOCKER_HOST: tcp://docker:2375
    DOCKER_DRIVER: overlay2
    DOCKER_TLS_CERTDIR: ""
  extends:
    - .default-job-settings


.run-unittests-and-lint:
  extends:
    - .unittests-and-lint-settings
  script:
    - section_start "Test Infrastructure"
    - python3 -m pytest ./Tests/scripts/infrastructure_tests/ -v --disable-warnings
    - python3 -m pytest ./Tests/Marketplace/Tests/ -v --disable-warnings
    - python3 -m pytest ./Tests/tests -v --disable-warnings
    - python3 -m pytest ./Tests/private_build/ -v --disable-warnings
    - python3 -m pytest Utils -v --disable-warnings
    - |
      if [ -n "${DEMISTO_SDK_NIGHTLY}" ]; then
        ./Tests/scripts/sdk_pylint_check.sh
      fi
    - section_end "Test Infrastructure"
    - section_start "Run Unit Testing and Lint"
    - |
      if [[ -n $FORCE_BUCKET_UPLOAD || -n $BUCKET_UPLOAD ]] && [[ "$(echo "$GCS_MARKET_BUCKET" | tr '[:upper:]' '[:lower:]')" != "marketplace-dist" ]] && [[ $CI_COMMIT_BRANCH != "master" ]]; then
        echo "Skipping validations when uploading to a test bucket."
      else
        echo "demisto-sdk version: $(demisto-sdk --version)"
        echo "mypy version: $(mypy --version)"
        echo "flake8 py3 version: $(python3 -m flake8 --version)"
        echo "bandit py3 version: $(python3 -m bandit --version 2>&1)"
        echo "vulture py3 version: $(python3 -m vulture --version 2>&1)"
        set -ex
        SHOULD_LINT_ALL=$(./Tests/scripts/should_lint_all.sh)
        echo "here before mkdir"
        pwd
        ls -lh
        mkdir ./unit-tests
        echo "here after mkdir"
        if [[ -n "${SHOULD_LINT_ALL}" || -n "${BUCKET_UPLOAD}" ]]; then
          echo "In should lint all / bucket upload."
          if [ $DOCKER == "native:dev,from-yml" ]; then
            echo "removing native:dev from docker list."
            DOCKER='from-yml'
          fi
        fi
        if [ -n "$SHOULD_LINT_ALL" ]; then
          echo -e  "----------\nLinting all because:\n${SHOULD_LINT_ALL}\n----------"
          demisto-sdk lint -p 10 -a --test-xml ./unit-tests --log-path $ARTIFACTS_FOLDER --failure-report $ARTIFACTS_FOLDER --coverage-report $ARTIFACTS_FOLDER/coverage_report -dt 120 --time-measurements-dir $ARTIFACTS_FOLDER --docker-image ${DOCKER}
        fi
        if [[ -n $BUCKET_UPLOAD ]]; then
          demisto-sdk lint --console-log-threshold DEBUG -p 8 -g --no-mypy --prev-ver $LAST_UPLOAD_COMMIT -v --test-xml ./unit-tests --log-path $ARTIFACTS_FOLDER --failure-report $ARTIFACTS_FOLDER --coverage-report $ARTIFACTS_FOLDER/coverage_report --check-dependent-api-module --docker-image ${DOCKER}
        else
          if [[ "$CI_COMMIT_BRANCH" != "master" ]]; then
            # skip running `lint -g` on master
            demisto-sdk lint --console-log-threshold DEBUG -p 8 -g --test-xml ./unit-tests --log-path ./artifacts --failure-report ./artifacts --coverage-report $ARTIFACTS_FOLDER/coverage_report --check-dependent-api-module --docker-image ${DOCKER}
          fi
        fi
      
        if [[ -f $ARTIFACTS_FOLDER/coverage_report/.coverage ]]; then
          if [[ "$CI_PIPELINE_SOURCE" == "schedule" ||   -n "$SHOULD_LINT_ALL"  ||  -n "${NIGHTLY}" || -n "${BUCKET_UPLOAD}" || -n "${DEMISTO_SDK_NIGHTLY}" ]]; then
            demisto-sdk coverage-analyze -i $ARTIFACTS_FOLDER/coverage_report/.coverage --report-dir $ARTIFACTS_FOLDER/coverage_report --report-type all --allowed-coverage-degradation-percentage 100
            if [[ -n "${NIGHTLY}" && "$CI_COMMIT_BRANCH" == "master" && $DOCKER == "from-yml" ]]; then
              python3 Utils/upload_code_coverage_report.py --service_account $GCS_MARKET_KEY --source_file_name $ARTIFACTS_FOLDER/coverage_report/coverage.json --minimal_file_name $ARTIFACTS_FOLDER/coverage_report/coverage-min.json
            fi
          else
            demisto-sdk coverage-analyze -i $ARTIFACTS_FOLDER/coverage_report/.coverage --report-dir $ARTIFACTS_FOLDER/coverage_report --report-type html,xml --previous-coverage-report-url https://storage.googleapis.com/marketplace-dist-dev/code-coverage-reports/coverage-min.json
          fi
        fi
      fi
    - section_end "Run Unit Testing and Lint"
    - job-done
  parallel:
    matrix:
      - DOCKER: ['native:ga,native:maintenance,native:candidate','native:dev,from-yml']

.run-validations:
  stage: unittests-and-validations
  extends:
    - .default-job-settings
  needs: []
  variables:
    KUBERNETES_CPU_REQUEST: 1000m
  artifacts:
    expire_in: 30 days
    paths:
      - ${CI_PROJECT_DIR}/artifacts/*
      - ${CI_PROJECT_DIR}/pipeline_jobs_folder/*
    when: always
  script:
    - section_start "Look For Secrets"
    - demisto-sdk secrets --post-commit --ignore-entropy
    - section_end "Look For Secrets"
    - section_start "Copy Tests To Artifact Folder"
    - cp "./Tests/conf.json" "$ARTIFACTS_FOLDER/conf.json"
    - section_end "Copy Tests To Artifact Folder"
    - section_start "Validate Files and Yaml"
    - |
      if [[ -n $FORCE_BUCKET_UPLOAD ]]; then
        echo "Skipping the -Validate Files and Yaml- step when force uploading to a bucket."
      else
        ./Tests/scripts/linters_runner.sh
        ./Tests/scripts/validate.sh
      fi
    - section_end "Validate Files and Yaml"
    - section_start "Check Spelling"
    - python3 ./Tests/scripts/circleci_spell_checker.py $CI_COMMIT_BRANCH
    - section_end "Check Spelling"
    - section_start "Validate landingPageSections.json"
    - echo "Download index.zip"
    - INDEX_PATH=$(mktemp)
    - |
      gcloud auth activate-service-account --key-file="$GCS_MARKET_KEY" >> $ARTIFACTS_FOLDER/logs/auth.out
      echo "successfully activated google cloud service account"
      gsutil cp "gs://marketplace-dist/content/packs/index.zip" $INDEX_PATH
      echo "successfully downloaded index.zip"
      gcloud auth revoke $GCS_ARTIFACTS_ACCOUNT_NAME
    - echo "successfully downloaded index.zip into $INDEX_PATH"

    - UNZIP_PATH=$(mktemp -d)
    - unzip $INDEX_PATH -d $UNZIP_PATH > $ARTIFACTS_FOLDER/logs/unzip_index.log

    - python3 Tests/Marketplace/validate_landing_page_sections.py -i $UNZIP_PATH
    - section_end "Validate landingPageSections.json"
    - !reference [ .validate_content_test_conf_branch_merged ]  # This section should be the last one in the script, do not move it.
    - job-done

.jobs-done-check:
  stage: are-jobs-really-done
  script:
    - python3 Tests/scripts/check_jobs_done.py --job-done-files $PIPELINE_JOBS_FOLDER
