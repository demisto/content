This integration leverages the OpenAI and Azure OpenAI REST APIs to generate human-like responses to text prompts. The APIs provide access to OpenAI's powerful language models including the GPT-3.5 model series. These models can be easily adapted to your specific task including but not limited to content generation, summarization, semantic search, and natural language to code translation.
This integration was integrated and tested with version 1 of the OpenAI and Azure OpenAI REST APIs.

Some changes have been made that might affect your existing content. 
If you are upgrading from a previous version of this integration, see [Breaking Changes](#breaking-changes-from-the-previous-version-of-this-integration-openai).

## Configure OpenAI v2 on Cortex XSOAR

1. Navigate to **Settings** > **Integrations** > **Servers & Services**.
2. Search for OpenAI.
3. Click **Add instance** to create and configure a new integration instance.

    | **Parameter** | **Description** | **Required** |
    | --- | --- | --- |
    | Server URL: (e.g. https://api.openai.com/) |  | True |
    | API Key | OpenAI API key | True |
    | Use Azure OpenAI? | Whether to use Azure OpenAI \(https://your-resource-name.openai.azure.com\) instead of standard OpenAI \(https://api.openai.com\) | False |
    | Trust any certificate (not secure) |  | False |
    | Use system proxy settings |  | False |
    | API Version | Used for Azure OpenAI only. Default value is "2023-03-15-preview". | False |
    | Model to Use for Test | Model to use for the integration Test if different from the default model, "text-davinci-003" | False |

4. Click **Test** to validate the URLs, token, and connection.

## Commands

You can execute these commands from the Cortex XSOAR CLI, as part of an automation, or in a playbook.
After you successfully execute a command, a DBot message appears in the War Room with the command details.

### openai-chatgpt

***
Send prompt to OpenAI ChatGPT

#### Base Command

`openai-chatgpt`

#### Input

| **Argument Name** | **Description** | **Required** |
| --- | --- | --- |
| prompt | Add your question or text. | Required | 
| model | Name of model to use. For Azure OpenAI, this should be the name of the DEPLOYMENT, not the generic model name. Possible values are: gpt-3.5-turbo. Default is gpt-3.5-turbo. | Optional | 

#### Context Output

| **Path** | **Type** | **Description** |
| --- | --- | --- |
| OpenAI.ChatGPTResponse.ChatGPT Response | unknown | ChatGPT response | 
| OpenAI.ChatGPTResponse.Created Time | unknown | Response created time as Unix timestamp | 
| OpenAI.ChatGPTResponse.Model | unknown | Model that generated the response | 
| OpenAI.ChatGPTResponse.Number of Completion Tokens | unknown | Number of tokens in the completion \(response\) | 
| OpenAI.ChatGPTResponse.Number of Prompt Tokens | unknown | Number of tokens in the prompt \(input\) | 
| OpenAI.ChatGPTResponse.Number of Total Tokens | unknown | Total number of tokens in the completion and prompt | 
| OpenAI.ChatGPTResponse.id | unknown | ChatGPT response ID | 

#### Command example
```!openai-chatgpt prompt=`Give detailed analysis of the following using the following form:\n1- Analysis (Incident Description)\n2- Impact Analysis\n3- Action or Recommendations (Must be detailed)\n\nThere is a security incident with the following information:\n1- Title: ${incident.name}\n2- Indicator of compromise: ${ExtractedIndicators}\n3- Indicator of compromise reputation: ${Indicators.Score}\n...` model="gpt-35-turbo"```
#### Context Example
```json
{
    "OpenAI": {
        "ChatGPTResponse": {
            "ChatGPT Response": "1- Analysis:\nThe security incident with title XDR Incident # - 'Local Analysis Malware' generated by XDR Agent detected on host ABC involving user XYZ indicates that the XDR agent on host ABC has detected local analysis malware on the host, and it is associated with user XYZ.\n\n2- Impact Analysis:\nAn incident like this can be critical for an organization as local analysis malware can give attackers unauthorized access to the system and sensitive data, compromising the confidentiality and integrity of the organization's systems and data. The impact of this incident can result in a significant loss or damage to the system and can severely impact the organization's reputation.\n\n3- Action or Recommendations:\nIt is of utmost importance to immediately respond to such an incident and take action to contain the malware and prevent further damage. Below are some recommendations that can help in addressing this incident:\n\n- First, isolate the infected machine from the network to contain the malware's spread.\n- Check other systems for similar indicators of compromise.\n- Perform a thorough analysis to identify the malware's behavior and its impact on the system.\n- If the malware is a known strain, make sure to scan all other systems and implement the appropriate patches and updates to eliminate it.\n- If a new strain of malware, submit it for analysis to security vendors and take action based on their recommendations.\n- Reset the user's password and monitor their activities.\n- Look for any signs of data exfiltration or other suspicious activities.\n- Train employees on how to be more vigilant and proactively identify potential security threats.\n\nBy following these recommendations, organizations can significantly reduce the impact of such incidents and protect their systems and data from further damage. It is essential to have an incident response plan in place to handle such incidents promptly and effectively. Organizations should also make sure to perform regular vulnerability and incident assessments to identify and address any potential security risks.",
            "Created Time": 1687968757,
            "Model": "gpt-35-turbo",
            "Number of Completion Tokens": 371,
            "Number of Prompt Tokens": 101,
            "Number of Total Tokens": 472,
            "id": "chatcmpl-123"
        }
    }
}
```

#### Human Readable Output

>### ChatGPT API Response
>|ChatGPT Response|Created Time|Model|Number of Completion Tokens|Number of Prompt Tokens|Number of Total Tokens|id|
>|---|---|---|---|---|---|---|
>| 1- Analysis:<br>The security incident with title XDR Incident # - 'Local Analysis Malware' generated by XDR Agent detected on host ABC involving user XYZ indicates that the XDR agent on host ABC has detected local analysis malware on the host, and it is associated with user XYZ.<br><br>2- Impact Analysis:<br>An incident like this can be critical for an organization as local analysis malware can give attackers unauthorized access to the system and sensitive data, compromising the confidentiality and integrity of the organization's systems and data. The impact of this incident can result in a significant loss or damage to the system and can severely impact the organization's reputation.<br><br>3- Action or Recommendations:<br>It is of utmost importance to immediately respond to such an incident and take action to contain the malware and prevent further damage. Below are some recommendations that can help in addressing this incident:<br><br>- First, isolate the infected machine from the network to contain the malware's spread.<br>- Check other systems for similar indicators of compromise.<br>- Perform a thorough analysis to identify the malware's behavior and its impact on the system.<br>- If the malware is a known strain, make sure to scan all other systems and implement the appropriate patches and updates to eliminate it.<br>- If a new strain of malware, submit it for analysis to security vendors and take action based on their recommendations.<br>- Reset the user's password and monitor their activities.<br>- Look for any signs of data exfiltration or other suspicious activities.<br>- Train employees on how to be more vigilant and proactively identify potential security threats.<br><br>By following these recommendations, organizations can significantly reduce the impact of such incidents and protect their systems and data from further damage. It is essential to have an incident response plan in place to handle such incidents promptly and effectively. Organizations should also make sure to perform regular vulnerability and incident assessments to identify and address any potential security risks. | 1970-01-20 12:52:48 | gpt-35-turbo | 371 | 101 | 472 | chatcmpl-7WS4T5dI4r0RR8ZRa19QCM0cERlKt |


### openai-completions

***
Enter an instruction and watch the API respond with a completion that attempts to match the context or pattern you provided.

#### Base Command

`openai-completions`

#### Input

| **Argument Name** | **Description** | **Required** |
| --- | --- | --- |
| prompt | Instruction. | Required | 
| model | The model which will generate the completion. Some models are suitable for natural language tasks, others specialize in code. For Azure OpenAI, this should be the name of the DEPLOYMENT, not the generic model name. Possible values are: text-davinci-003, text-curie-001, text-babbage-001, text-ada-001, code-davinci-002, code-cushman-001. Default is text-davinci-003. | Optional | 
| temperature | Controls randomness: Lowering results in less random completions. Default is 0.2. | Optional | 
| max_tokens | The maximum number of tokens to generate. Default is 256. | Optional | 
| top_p | Controls Diversity via nucleus sampling: 0.5 means half of all likihood-weighted options are considered. Default is 1. | Optional | 
| frequency_penalty | How much to penalize new tokens based on their existing frequency in the text so far. Decreases the model's likelihood to repeat the same line verbatim. Default is 0. | Optional | 
| presence_penalty | How much to penalize new tokens based on whether they appear in the text so far. Increases the model's likelihood to talk about new topics. Default is 0. | Optional | 
| best_of | Generates best_of completions server-side and returns the "best" (the one with the lowest log probability per token). Results can't be streamed. When used with n, best_of controls the number of candidate completions and n specifies how many to return â€“ best_of must be greater than n. Note- Because this parameter generates many completions, it can quickly consume your token quota. Use carefully and ensure that you have reasonable settings for max_tokens and stop. This parameter cannot be used with gpt-35-turbo. Default is 1. | Optional | 
| stop | Up to four sequences where the API will stop generating further tokens. The returned text won't contain the stop sequence. | Optional | 

#### Context Output

| **Path** | **Type** | **Description** |
| --- | --- | --- |
| OpenAI.Completions.id | string | Id of the returned completion. | 
| OpenAI.Completions.model | string | The model which will generate the completion. | 
| OpenAI.Completions.text | string | Completed text generated by OpenAI? | 

#### Command example
```!openai-completions prompt="Give me some characteristics of a phishing email"```
#### Context Example
```json
{
    "OpenAI": {
        "Completions": {
            "id": "cmpl-7WRxVmzy5SoGNpus2lfau6oik29li",
            "model": "text-davinci-003",
            "text": "\n\n1. Unfamiliar sender: The sender of the email may be someone you don't recognize or an email address that looks suspicious.\n\n2. Urgent requests: The email may contain urgent requests for you to take action, such as clicking a link or providing personal information.\n\n3. Poor grammar and spelling: Phishing emails often contain poor grammar and spelling mistakes.\n\n4. Suspicious links: The email may contain links to websites that look legitimate but are actually malicious.\n\n5. Attachments: The email may contain attachments that could contain malicious software."
        }
    }
}
```

#### Human Readable Output

>### OpenAI - Completions
>Model text-davinci-003 generated 1 possible text completion(s).
>|id|model|text|
>|---|---|---|
>| cmpl-7WRxVmzy5SoGNpus2lfau6oik29li | text-davinci-003 | <br/><br/>1. Unfamiliar sender: The sender of the email may be someone you don't recognize or an email address that looks suspicious.<br/><br/>2. Urgent requests: The email may contain urgent requests for you to take action, such as clicking a link or providing personal information.<br/><br/>3. Poor grammar and spelling: Phishing emails often contain poor grammar and spelling mistakes.<br/><br/>4. Suspicious links: The email may contain links to websites that look legitimate but are actually malicious.<br/><br/>5. Attachments: The email may contain attachments that could contain malicious software. |


### openai-answer-question

***
Ask the model a question to answer based on input data

#### Base Command

`openai-answer-question`

#### Input

| **Argument Name** | **Description** | **Required** |
| --- | --- | --- |
| question | Question to ask the model. | Required | 
| text | Data for the model to use to answer the question. Either arg "text" or "entry_id" is required. | Optional | 
| entry_id | Entry ID of file for the model to use to answer the question. Either arg "text" or "entry_id" is required. | Optional | 
| model | Name of model to use. Used for Azure OpenAI only. Default is text-davinci-003. | Optional | 
| deployment | Name of deployment to use. Used for Azure OpenAI only. Default is text-davinci-003. | Optional | 
| temperature | Controls randomness: Lowering results in less random completions. Default is 0.2. | Optional | 
| chat_history | Chat history to pass into the model to inform the answer to the current question. Must be in the format of the ${OpenAI.QA} context key from a previous output of the `openai-answer-question` command. | Optional | 

#### Context Output

| **Path** | **Type** | **Description** |
| --- | --- | --- |
| OpenAI.QA.Question | unknown | Question asked to the model | 
| OpenAI.QA.Answer | unknown | Answer provided by the model | 

#### Command example
```!openai-answer-question question=`What are some XSOAR playbook best practices?` entry_id=1504@3d4f3713-0634-41eb-833b-299c6253eea9```
#### Context Example
```json
{
    "OpenAI": {
        "QA": {
            "Answer": " Some XSOAR playbook best practices include using preprocessing to drop low value alerts, using quiet mode and disabling auto-extract on playbook tasks, and reviewing common indicators and deleting/excluding known good indicators.",
            "Question": "What are some XSOAR playbook best practices?"
        }
    }
}
```

#### Human Readable Output

>**Answer**:  Some XSOAR playbook best practices include using preprocessing to drop low value alerts, using quiet mode and disabling auto-extract on playbook tasks, and reviewing common indicators and deleting/excluding known good indicators.

### openai-summarize

***
Summarize the data provided to the model

#### Base Command

`openai-summarize`

#### Input

| **Argument Name** | **Description** | **Required** |
| --- | --- | --- |
| text | Text for the model to summarize. Either arg "text" or "entry_id" is required. | Optional | 
| entry_id | Entry ID of file for the model to summarize. Either arg "text" or "entry_id" is required. | Optional | 
| prompt | Additional prompt to optionally add more specific instructions. | Optional | 
| model | Name of model to use. Used for Azure OpenAI only. Default is text-davinci-003. | Optional | 
| deployment | Name of deployment to use. Used for Azure OpenAI only. Default is text-davinci-003. | Optional | 
| temperature | Controls randomness: Lowering results in less random completions. Default is 0. | Optional | 

#### Context Output

| **Path** | **Type** | **Description** |
| --- | --- | --- |
| OpenAI.Summary | unknown | Summary returned by the model | 

#### Command example
```!openai-summarize entry_id=1504@3d4f3713-0634-41eb-833b-299c6253eea9```
#### Context Example
```json
{
    "OpenAI": {
        "Summary": "This guide provides best practices for optimizing the performance of Cortex XSOAR, a security orchestration, automation, and response platform. It includes configuration best practices, such as enabling signature verification on incoming SAML authentication tokens, restricting workers for all time searches, and database optimizations. Additionally, it provides help on playbook design and best practices, such as using Quiet Mode and disabling auto-extract on tasks that do not require it. It also provides instructions on how to configure a server to restrict the number of workers allowed to execute parallel searches, as well as how to audit index tuning and purge existing items. Additionally, it provides guidance on how to administer, tune, monitor, and maintain Cortex XSOAR for multi-tenant hosts and HA groups."
    }
}
```

#### Human Readable Output

>**Summary**: This guide provides best practices for optimizing the performance of Cortex XSOAR, a security orchestration, automation, and response platform. It includes configuration best practices, such as enabling signature verification on incoming SAML authentication tokens, restricting workers for all time searches, and database optimizations. Additionally, it provides help on playbook design and best practices, such as using Quiet Mode and disabling auto-extract on tasks that do not require it. It also provides instructions on how to configure a server to restrict the number of workers allowed to execute parallel searches, as well as how to audit index tuning and purge existing items. Additionally, it provides guidance on how to administer, tune, monitor, and maintain Cortex XSOAR for multi-tenant hosts and HA groups.

## Additional Considerations for this version
* This integration combines two previously existing integrations: `OpenAI` and `OpenAi ChatGPT v3`.
* The command *chatgpt-send-prompt* (in integration `OpenAi ChatGPT v3`) is now the *openai-chatgpt* commmand.
* This integration supports both OpenAI (api.openai.com) and Azure OpenAI. Ensure you have the 'Use Azure OpenAI?' checkbox selected if you want to use Azure.
* This integration supports both the Chat endpoint (via command *openai-chatgpt*) and Completions endpoint (via command *openai-completions*). Ensure you are using the intended command.
* For longer inputs that would otherwise exceed the token limit, use commands *openai-answer-question* or *openai-summarize*. These commands embed the input so it can be queried more efficiently.
* For use cases where memory of chat history is required, use command *openai-answer-question* with argument *chat_history*.
