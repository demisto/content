category: Analytics & SIEM
sectionOrder:
- Connect
- Collect
commonfields:
  id: Tenable.io Event Collector Test
  version: -1
configuration:
- additionalinfo: This is the API endpoint for the Tenable.io API.
  defaultvalue: https://cloud.tenable.com
  display: Server URL
  name: url
  required: true
  type: 0
  section: Connect
- displaypassword: Secret Key
  display: Access Key
  additionalinfo: Tenable API access and secret key.
  name: credentials
  required: true
  type: 9
  section: Connect
- display: Events
  name: events
  type: 20
  section: Collect
- display: Severity
  additionalinfo: The severity of the vulnerabilities to include in the export.
  name: severity
  options:
  - info
  - low
  - medium
  - high
  - critical
  type: 16
  required: false
  section: Collect
- defaultvalue: 7 days
  display: First fetch timestamp (<number> <time unit>, e.g., 12 hours, 7 days)
  name: first_fetch
  type: 0
  required: false
  section: Collect
- additionalinfo: The maximum number of audit logs to retrieve for each event type. For more information about event types see the help section.
  defaultvalue: '1000'
  display: Max Fetch
  name: max_fetch
  type: 0
  required: false
  section: Collect
- display: Trust any certificate (not secure)
  name: insecure
  type: 8
  required: false
  section: Connect
- display: Use system proxy settings
  name: proxy
  type: 8
  required: false
  section: Connect
- display: Vulnerabilities Fetch Interval in minutes
  additionalinfo: Time in minutes between fetches of vulnerabilities.
  name: vuln_fetch_interval
  required: true
  type: 0
  section: Collect
- display: Assets
  name: assets
  type: 20
  section: Collect
- defaultvalue: 7 days
  display: Assets first fetch timestamp (<number> <time unit>, e.g., 12 hours, 7 days)
  name: first_fetch_time_assets
  required: false
  type: 0
  section: Collect
- additionalinfo: The maximum number of assets to retrieve.
  defaultvalue: '5000'
  display: Max Assets Fetch
  name: max_assets_fetch
  required: false
  type: 0
  section: Collect
description: Use Tenable.io Event Collector integration to get Audit and Endpoint logs, assets and their vulnerabilities from Tenable.
display: Tenable.io Event Collector Test
name: Tenable.io Event Collector Test
script:
  commands:
  - arguments:
    - auto: PREDEFINED
      defaultValue: 'false'
      description: Set this argument to True in order to create events, otherwise the command will only display the events.
      name: should_push_events
      predefined:
      - 'true'
      - 'false'
      required: true
    - description: The maximum number of alerts to return (maximum value - 5000).
      name: limit
    - description: Return events that occurred after the specified date.
      name: from_date
    - description: Return events that occurred before the specified date.
      name: to_date
    - description: Return events that contain the specified actor UUID.
      name: actor_id
    - description: Return events matching the specified target UUID.
      name: target_id
    description: Returns audit logs extracted from Tenable io.
    name: tenable-get-audit-logs
  - arguments:
    - auto: PREDEFINED
      defaultValue: 'false'
      description: Set this argument to True in order to create events, otherwise the command will only display the events.
      name: should_push_events
      predefined:
      - 'true'
      - 'false'
      required: true
    - description: Returns vulnerabilities that were last found between the specified date (in Unix time) and now.
      name: last_found
    - description: The severity of the vulnerabilities to include in the export.
      name: num_assets
    - name: hide_polling_output
      description: Whether to hide the polling output.
      deprecated: true
    - name: export_uuid
      description: Whether to export the UUID for which to check the status.
      deprecated: true
    description: Returns vulnerabilities extracted from Tenable io.
    polling: true
    name: tenable-get-vulnerabilities
  - arguments:
    - default: false
      description: Returns assets that were last found between the specified date (in Unix time) and now.
      isArray: false
      name: last_fetch
      required: false
      secret: false
    - default: false
      description: The number of the assets per chunk.
      isArray: false
      name: chunk_size
      required: false
      secret: false
    - name: hide_polling_output
      description: Whether to hide the polling output.
      deprecated: true
    - name: export_uuid
      description: Whether to export the UUID for which to check the status.
      deprecated: true
    deprecated: false
    description: Returns assets extracted from Tenable io.
    execution: false
    polling: true
    name: tenable-export-assets
  isfetch: false
  runonce: false
  script: >
    register_module_line('Tenable.io Event Collector Test', 'start', __line__())

    ### pack version: 2.1.13



    import time

    from typing import Dict, Union


    import urllib3


    urllib3.disable_warnings()


    ''' CONSTANTS '''


    MAX_EVENTS_PER_REQUEST = 100

    VENDOR = 'tenable'

    PRODUCT = 'io'

    NUM_ASSETS = 5000

    DATE_FORMAT = '%Y-%m-%d'

    MAX_CHUNK_SIZE = 5000


    ''' CLIENT CLASS '''



    class Client(BaseClient):
        """Client class to interact with the service API

            This Client implements API calls to the Saas Security platform, and does not contain any XSOAR logic.
            Handles the token retrieval.

            :param base_url (str): Saas Security server url.
            :param client_id (str): client ID.
            :param client_secret (str): client secret.
            :param verify (bool): specifies whether to verify the SSL certificate or not.
            :param proxy (bool): specifies if to use XSOAR proxy settings.
            """

        @staticmethod
        def add_query(query, param_to_add):
            if query:
                return f'{query}&{param_to_add}'
            return f'?{param_to_add}'

        def get_audit_logs_request(self, from_date: str = None, to_date: str = None, actor_id: str = None,
                                   target_id: str = None, limit: int = None):
            """

            Args:
                limit: limit number of audit logs to get.
                from_date: date to fetch audit logs from.
                to_date: date which until to fetch audit logs.
                actor_id: fetch audit logs with matching actor id.
                target_id:fetch audit logs with matching target id.

            Returns:
                audit logs fetched from the API.
            """
            query = ''
            if from_date:
                query = self.add_query(query, f'f=date.gt:{from_date}')
            if to_date:
                query = self.add_query(query, f'f=date.lt:{to_date}')
            if actor_id:
                query = self.add_query(query, f'f=actor_id.match:{actor_id}')
            if target_id:
                query = self.add_query(query, f'f=target_id.match:{target_id}')
            if limit:
                query = self.add_query(query, f'limit={limit}')
            else:
                query = self.add_query(query, 'limit=5000')
            res = self._http_request(method='GET', url_suffix=f'/audit-log/v1/events{query}', headers=self._headers)
            return res.get('events', [])

        def get_export_uuid(self, num_assets: int, last_found: Optional[float], severity: List[str]):
            """

            Args:
                num_assets: number of assets used to chunk the vulnerabilities.
                last_found: vulnerabilities that were last found between the specified date (in Unix time) and now.
                severity: severity of the vulnerabilities to include in the export.

            Returns: The UUID of the vulnerabilities export job.

            """
            payload: Dict[str, Union[Any]] = {
                "filters":
                    {
                        "severity": severity
                    },
                "num_assets": num_assets
            }
            if last_found:
                payload['filters'].update({"last_found": last_found})

            res = self._http_request(method='POST', url_suffix='/vulns/export', headers=self._headers, json_data=payload)
            return res.get('export_uuid', '')

        def get_export_status(self, export_uuid: str):
            """

            Args:
                export_uuid: The UUID of the vulnerabilities export job.

            Returns: The status of the job, and number of chunks available if succeeded.

            """
            res = self._http_request(method='GET', url_suffix=f'/vulns/export/{export_uuid}/status',
                                     headers=self._headers)
            status = res.get('status')
            chunks_available = res.get('chunks_available', [])
            return status, chunks_available

        def download_vulnerabilities_chunk(self, export_uuid: str, chunk_id: int):
            """

            Args:
                export_uuid: The UUID of the vulnerabilities export job.
                chunk_id: The ID of the chunk you want to export.

            Returns: Chunk of vulnerabilities from API.

            """
            return self._http_request(method='GET', url_suffix=f'/vulns/export/{export_uuid}/chunks/{chunk_id}',
                                      headers=self._headers)

        def export_assets_request(self, chunk_size: int, fetch_from):
            """

            Args:
                chunk_size: maximum number of assets in one chunk.
                fetch_from: the last asset that was fetched previously.

            Returns: The UUID of the assets export job.

            """
            payload = {
                'chunk_size': chunk_size,
                "filters": {
                    "created_at": fetch_from
                }
            }
            demisto.debug(f"my payload is: {payload}")
            res = self._http_request(method='POST', url_suffix='assets/export', json_data=payload,
                                     headers=self._headers)
            return res.get('export_uuid')

        def get_export_assets_status(self, export_uuid):
            """
            Args:
                    export_uuid: The UUID of the assets export job.

            Returns: The assets' chunk id.

            """
            res = self._http_request(method='GET', url_suffix=f'assets/export/{export_uuid}/status', headers=self._headers)
            return res.get('status'), res.get('chunks_available')

        def download_assets_chunk(self, export_uuid: str, chunk_id: int):
            """

            Args:
                export_uuid: The UUID of the assets export job.
                chunk_id: The ID of the chunk you want to export.

            Returns: Chunk of assets from API.

            """
            return self._http_request(method='GET', url_suffix=f'/assets/export/{export_uuid}/chunks/{chunk_id}',
                                      headers=self._headers)


    ''' HELPER FUNCTIONS '''



    def get_timestamp(timestamp):
        return time.mktime(timestamp.timetuple())


    def try_get_chunks(client: Client, export_uuid: str):
        """
        If job has succeeded (status FINISHED) get all information from all chunks available.
        Args:
            client: Client class object.
            export_uuid: The UUID of the vulnerabilities export job.

        Returns: All information from all chunks available.

        """
        vulnerabilities = []
        status, chunks_available = client.get_export_status(export_uuid=export_uuid)
        demisto.info(f'Report status is {status}, and number of available chunks is {chunks_available}')
        if status == 'FINISHED':
            for chunk_id in chunks_available:
                vulnerabilities.extend(client.download_vulnerabilities_chunk(export_uuid=export_uuid, chunk_id=chunk_id))
        return vulnerabilities, status


    def try_get_assets_chunks(client: Client, export_uuid: str):
        """
        If job has succeeded (status FINISHED) get all information from all chunks available.
        Args:
            client: Client class object.
            export_uuid: The UUID of the assets export job.

        Returns: All information from all chunks available.

        """
        assets = []
        status, chunks_available = client.get_export_assets_status(export_uuid=export_uuid)
        demisto.info(f'Report status is {status}, and number of available chunks is {chunks_available}')
        if status == 'FINISHED':
            for chunk_id in chunks_available:
                assets.extend(client.download_assets_chunk(export_uuid=export_uuid, chunk_id=chunk_id))
        return assets, status


    def generate_export_uuid(client: Client, first_fetch: datetime, last_run: Dict[str, Union[str, float, None]],
                             severity: List[str]):
        """
        Generate a job export uuid in order to fetch vulnerabilities.

        Args:
            client: Client class object.
            first_fetch: time to first fetch from.
            last_run: last run object.
            severity: severity of the vulnerabilities to include in the export.

        """
        demisto.info("Getting export uuid for report.")
        last_fetch = last_run.get('last_fetch_vuln')
        last_found: float = last_fetch or get_timestamp(first_fetch)
        # last_found: float = time.mktime(
        #     first_fetch.timetuple()) if not last_fetch and first_fetch else last_fetch  # type: ignore
        export_uuid = client.get_export_uuid(num_assets=NUM_ASSETS, last_found=last_found, severity=severity)

        next_run_vuln = get_timestamp(datetime.now(tz=timezone.utc))
        demisto.info(f'export uuid is {export_uuid}')
        last_run.update({'last_found_use': last_found, 'last_fetch_vuln': next_run_vuln, 'export_uuid': export_uuid})


    def run_vulnerabilities_fetch(last_run, first_fetch: datetime,
                                  vuln_fetch_interval: int):
        """

        Args:
            last_run: last run object.
            first_fetch: time to first fetch from.
            vuln_fetch_interval: vulnerabilities fetch interval.

        Returns: True if fetch vulnerabilities interval time has passed since last time that fetch run.

        """
        if not last_run.get('last_fetch_vuln'):
            time_to_check: float = get_timestamp(first_fetch)
        else:
            time_to_check = last_run['last_fetch_vuln']
        return time.time() - time_to_check > vuln_fetch_interval and not last_run.get('export_uuid')


    def insert_type_to_logs(audit_logs: list, vulnerabilities: list):
        """
        In order for the user to get easy access to events in the system based on their type, the type of the event is added
        manually.

        Args:
            audit_logs: audit logs to add xsiam type to.
            vulnerabilities: vulnerabilities to add xsiam type to.

        """
        for log in audit_logs:
            log.update({'xsiam_type': 'audit_log'})
        for log in vulnerabilities:
            log.update({'xsiam_type': 'vulnerability'})


    def call_send_events_to_xsiam(events, vulnerabilities, should_push_events=False):
        """Enhanced and sends events and vulnerabilities to XSIAM"""
        insert_type_to_logs(audit_logs=events, vulnerabilities=vulnerabilities)
        if should_push_events:
            send_events_to_xsiam(events, vendor=VENDOR, product=PRODUCT)
            send_events_to_xsiam(vulnerabilities, vendor=VENDOR, product=PRODUCT)


    ''' COMMAND FUNCTIONS '''



    def get_audit_logs_command(client: Client, from_date: Optional[str] = None, to_date: Optional[str] = None,
                               actor_id: Optional[str] = None, target_id: Optional[str] = None,
                               limit: Optional[int] = None):
        """

        Args:
            client: Client class object.
            from_date: date to fetch audit logs from.
            to_date: date which until to fetch audit logs.
            actor_id: fetch audit logs with matching actor id.
            target_id:fetch audit logs with matching target id.
            limit: limit number of audit logs to get.

        Returns: CommandResults of audit logs from API.

        """
        audit_logs = client.get_audit_logs_request(from_date=from_date,
                                                   to_date=to_date,
                                                   actor_id=actor_id,
                                                   target_id=target_id,
                                                   limit=limit)

        readable_output = tableToMarkdown('Audit Logs List:', audit_logs,
                                          removeNull=True,
                                          headerTransform=string_to_table_header)

        results = CommandResults(readable_output=readable_output,
                                 raw_response=audit_logs)
        return results, audit_logs


    @polling_function('tenable-get-vulnerabilities', requires_polling_arg=False)

    def get_vulnerabilities_command(args: Dict[str, Any], client: Client) -> Union[CommandResults, PollResult]:
        """
        Getting vulnerabilities from Tenable. Polling as long as the report is not ready (status FINISHED or failed)
        Args:
            args: arguments from user (last_found, severity and num_assets)
            client: Client class object.

        Returns: Vulnerabilities from API.

        """
        vulnerabilities = []
        last_found = arg_to_number(args.get('last_found'))
        num_assets = arg_to_number(args.get('num_assets')) or 5000
        severity = argToList(args.get('severity'))
        export_uuid = args.get('export_uuid')
        if not export_uuid:
            export_uuid = client.get_export_uuid(num_assets=num_assets, last_found=last_found,
                                                 severity=severity)  # type: ignore

        status, chunks_available = client.get_export_status(export_uuid=export_uuid)
        if status == 'FINISHED':
            for chunk_id in chunks_available:
                vulnerabilities.extend(client.download_vulnerabilities_chunk(export_uuid=export_uuid, chunk_id=chunk_id))
            readable_output = tableToMarkdown('Vulnerabilities List:', vulnerabilities,
                                              removeNull=True,
                                              headerTransform=string_to_table_header)

            results = CommandResults(readable_output=readable_output,
                                     raw_response=vulnerabilities)
            return PollResult(response=results)
        elif status in ['CANCELLED', 'ERROR']:
            results = CommandResults(readable_output='Export job failed',
                                     entry_type=entryTypes['error'])
            return PollResult(response=results)
        else:
            results = CommandResults(readable_output='Export job failed',
                                     entry_type=entryTypes['error'])
            return PollResult(continue_to_poll=True, args_for_next_run={"export_uuid": export_uuid, **args},
                              response=results)


    @polling_function('tenable-export-assets', requires_polling_arg=False)

    def get_assets_command(client: Client, args: Dict[str, Any]):
        """
        Getting assets from Tenable. Polling as long as the report is not ready (status FINISHED or failed)
        Args:
            args: arguments from user (last_found, severity and num_assets)
            client: Client class object.

        Returns: assets from API.

        """
        fetch_from = arg_to_number(args.get('last_fetch'))
        max_fetch = arg_to_number(args.get('chunk_size')) or MAX_CHUNK_SIZE
        export_uuid = args.get('export_uuid')
        assets = []
        if not export_uuid:
            export_uuid = client.export_assets_request(chunk_size=MAX_CHUNK_SIZE, fetch_from=fetch_from)

        status, chunks_available = client.get_export_assets_status(export_uuid=export_uuid)

        # if getting chunks from API has finished, or we reached the max amount required
        if status == 'FINISHED' or MAX_CHUNK_SIZE * len(chunks_available) < max_fetch:
            for chunk_id in chunks_available:
                assets.extend(client.download_assets_chunk(export_uuid=export_uuid, chunk_id=chunk_id))
            readable_output = tableToMarkdown('Assets List:', assets,
                                              removeNull=True,
                                              headerTransform=string_to_table_header)

            results = CommandResults(readable_output=readable_output,
                                     raw_response=assets)
            return PollResult(response=results)
        elif status in ['CANCELLED', 'ERROR']:
            results = CommandResults(readable_output='Assets export job failed',
                                     entry_type=entryTypes['error'])
            return PollResult(response=results)
        else:
            results = CommandResults(readable_output='Export job failed',
                                     entry_type=entryTypes['error'])
            return PollResult(continue_to_poll=True, args_for_next_run={"export_uuid": export_uuid, **args},
                              response=results)


    def generate_assets_export_uuid(client: Client, first_fetch: datetime, assets_last_run: Dict[str, Union[str, float, None]]):
        """
        Generate a job export uuid in order to fetch assets.

        Args:
            client: Client class object.
            first_fetch: time to first fetch assets from.
            assets_last_run: assets last run object.

        """

        demisto.info("Getting export uuid.")

        last_fetch: int = assets_last_run.get('last_fetch') or round(get_timestamp(first_fetch))
        export_uuid = client.export_assets_request(chunk_size=MAX_CHUNK_SIZE, fetch_from=last_fetch)    # todo: to keep chunk size as 5000 max?
        demisto.debug(f'assets export uuid is {export_uuid}')

        assets_last_run.update({'last_fetch': last_fetch, 'export_uuid': export_uuid})


    def handle_finished_status(assets_last_run, assets):
        last_asset_id = assets_last_run.get('asset_id')
        last_fetch = assets_last_run.get('last_fetch')
        if last_asset_id:
            assets = list(filter(lambda asset: asset.get('id') != last_asset_id, assets))
        assets_last_run.update({'export_uuid': None})  # if the polling is over and we can start a new fetch
        assets_last_run.pop('nextTrigger', None)
        for asset in assets:
            created_at = round(get_timestamp(arg_to_datetime(asset.get('created_at'))))
            if created_at > last_fetch:
                last_fetch = created_at
                assets_last_run.update({"asset_id": asset.get("id")})
        assets_last_run.update({'last_fetch': last_fetch})

        return assets, assets_last_run


    ''' FETCH COMMANDS '''



    def fetch_assets_command(client: Client, assets_last_run, max_fetch):   # todo: add a use to max_fetch
        """
        Fetches assets.
        Args:
            assets_last_run: last run object.
            client: Client class object.

        Returns:
            assets fetched from the API.
        """
        assets = []
        export_uuid = assets_last_run.get('export_uuid')    # if already in assets_last_run meaning its still polling chunks from api
        last_fetch = assets_last_run.get('last_fetch')  # assets last run fetch time
        if export_uuid:
            demisto.info(f'Got export uuid from API {export_uuid}')
            assets, status = try_get_assets_chunks(client=client, export_uuid=export_uuid)
            # status = 'inProgress'
            # if status == 'inProgress':
            #     demisto.debug("status is in progress, merit test")
            #     assets_last_run.update({'nextTrigger': '30', 'type': 1})
            # set params for next run
            if status == 'FINISHED':
                assets, assets_last_run = handle_finished_status(assets_last_run, assets)
                # last_asset_id = assets_last_run.get('asset_id')
                # if last_asset_id:
                #     assets = list(filter(lambda asset: asset.get('id') != last_asset_id, assets))
                # assets_last_run.update({'export_uuid': None})   # if the polling is over and we can start a new fetch
                # assets_last_run.pop('nextTrigger', None)
                # assets_last_run.pop('type', None)
                # for asset in assets:
                #     created_at = round(get_timestamp(arg_to_datetime(asset.get('created_at'))))
                #     if created_at > last_fetch:
                #         last_fetch = created_at
                #         assets_last_run.update({"asset_id": asset.get("id")})
                # assets_last_run.update({'last_fetch': last_fetch})
                assets = assets[:max_fetch] if len(assets) > max_fetch else assets
            elif status in ['CANCELLED', 'ERROR'] and last_fetch:
                export_uuid = client.export_assets_request(chunk_size=2, fetch_from=last_fetch)
                assets_last_run.update({'export_uuid': export_uuid})
                assets_last_run.update({'nextTrigger': '30'})

        demisto.info(f'merit, Done fetching {len(assets)} assets, {assets_last_run=}.')
        return assets, assets_last_run


    def fetch_vulnerabilities(client: Client, last_run: dict, severity: List[str]):
        """
        Fetches vulnerabilities if job has succeeded.
        Args:
            last_run: last run object.
            severity: severity of the vulnerabilities to include in the export.
            client: Client class object.

        Returns:
            Vulnerabilities fetched from the API.
        """
        vulnerabilities = []
        export_uuid = last_run.get('export_uuid')
        last_found_use = last_run.get('last_found_use')  # last run fetch time
        if export_uuid:
            demisto.info(f'Got export uuid from API {export_uuid}')
            vulnerabilities, status = try_get_chunks(client=client, export_uuid=export_uuid)
            # set params for next run
            if status == 'FINISHED':
                last_run.update({'export_uuid': None})
            elif status in ['CANCELLED', 'ERROR'] and last_found_use:
                export_uuid = client.get_export_uuid(num_assets=5000, last_found=last_found_use, severity=severity)
                last_run.update({'export_uuid': export_uuid})

        demisto.info(f'Done fetching {len(vulnerabilities)} vulnerabilities, {last_run=}.')
        return vulnerabilities


    def fetch_events_command(client: Client, first_fetch: datetime, last_run: dict, limit: int = 1000):
        """
        Fetches audit logs.
        Args:
            client: Client class object.
            first_fetch: time to first fetch from.
            last_run: last run object.
            limit: number of audit logs to max fetch.

        Returns: vulnerabilities, audit logs and updated last run object

        """

        last_fetch = last_run.get('last_fetch_time')
        last_index_fetched = last_run.get('index_audit_logs', 0)
        if not last_fetch:
            start_date = first_fetch.strftime(DATE_FORMAT)
        else:
            start_date = last_fetch  # type: ignore

        audit_logs: List[Dict] = []
        audit_logs_from_api = client.get_audit_logs_request(from_date=start_date)

        if last_index_fetched < len(audit_logs_from_api):
            audit_logs.extend(audit_logs_from_api[last_index_fetched:last_index_fetched + limit])

        next_run: str = datetime.now(tz=timezone.utc).strftime(DATE_FORMAT)
        last_run.update({'index_audit_logs': len(audit_logs) + last_index_fetched if audit_logs else last_index_fetched,
                         'last_fetch_time': next_run})
        demisto.info(f'Done fetching {len(audit_logs)} audit logs, Setting {last_run=}.')
        return audit_logs, last_run


    def test_module(client: Client) -> str:
        """Tests API connectivity and authentication'

        Returning 'ok' indicates that the integration works like it is supposed to.
        Connection to the service is successful.
        Raises exceptions if something goes wrong.

        :type client: ``Client``
        :param Client: client to use

        :return: 'ok' if test passed, anything else will fail the test.
        :rtype: ``str``
        """

        client.get_audit_logs_request(limit=10)
        return 'ok'


    ''' MAIN FUNCTION '''



    def main() -> None:  # pragma: no cover
        """main function, parses params and runs command functions
        """
        args = demisto.args()
        command = demisto.command()
        params = demisto.params()
        events = []
        vulnerabilities: list = []

        access_key = params.get('credentials', {}).get('identifier', '')
        secret_key = params.get('credentials', {}).get('password', '')
        url = params.get('url')
        vuln_fetch_interval = arg_to_number(params.get('vuln_fetch_interval', 240)) * 60  # type: ignore
        severity = argToList(params.get('severity'))

        verify_certificate = not params.get('insecure', False)
        proxy = params.get('proxy', False)
        max_fetch = arg_to_number(params.get('max_fetch')) or 1000
        max_assets_fetch = arg_to_number(params.get("max_assets_fetch"))  # todo: how to use it? and how much is the max?

        # transform minutes to seconds
        first_fetch: datetime = arg_to_datetime(params.get('first_fetch', '3 days'))  # type: ignore
        first_assets_fetch: datetime = arg_to_datetime(params.get("first_fetch_time_assets", "3 years"))

        demisto.debug(f'Command being called is {command}')
        try:
            headers = {'X-ApiKeys': f'accessKey={access_key}; secretKey={secret_key}',
                       "Accept": "application/json"}
            client = Client(
                base_url=url,
                verify=verify_certificate,
                headers=headers,
                proxy=proxy)

            if command == 'test-module':
                return_results(test_module(client))

            elif command == 'tenable-get-audit-logs':
                results, events = get_audit_logs_command(client,
                                                         from_date=args.get('from_date'),
                                                         to_date=args.get('to_date'),
                                                         actor_id=args.get('actor_id'),
                                                         target_id=args.get('target_id'),
                                                         limit=args.get('limit'))
                return_results(results)

                call_send_events_to_xsiam(events=events, vulnerabilities=vulnerabilities,
                                          should_push_events=argToBoolean(args.get('should_push_events', 'true')))

            elif command == 'tenable-get-vulnerabilities':
                results = get_vulnerabilities_command(args, client)
                if isinstance(results, CommandResults):
                    if results.raw_response:
                        vulnerabilities = results.raw_response  # type: ignore
                return_results(results)

                call_send_events_to_xsiam(events=events, vulnerabilities=vulnerabilities,
                                          should_push_events=argToBoolean(args.get('should_push_events', 'true')))

            elif command == 'fetch-events':
                pass
                # last_run = demisto.getLastRun()
                # if run_vulnerabilities_fetch(last_run=last_run, first_fetch=first_fetch,
                #                              vuln_fetch_interval=vuln_fetch_interval):
                #     generate_export_uuid(client, first_fetch, last_run, severity)
                #
                # vulnerabilities = fetch_vulnerabilities(client, last_run, severity)
                # events, new_last_run = fetch_events_command(client, first_fetch, last_run, max_fetch)
                #
                # call_send_events_to_xsiam(events=events, vulnerabilities=vulnerabilities, should_push_events=True)
                #
                # demisto.debug(f'Setting new last_run to {new_last_run}')
                # demisto.setLastRun(new_last_run)

            elif command == 'fetch-assets':

                assets_last_run = demisto.getAssetsLastRun()
                demisto.debug(f"saved assets lastrun: {assets_last_run}")

                # starting new fetch for assets, not polling from prev call
                if not assets_last_run.get('export_uuid'):
                    generate_assets_export_uuid(client, first_assets_fetch, assets_last_run)

                assets, new_assets_last_run = fetch_assets_command(client, assets_last_run, max_fetch)
                demisto.debug(f"Done fetching {len(assets)} assets, sending to xsiam.")
                demisto.setAssetsLastRun(new_assets_last_run)

                # todo: to be implemented in CSP once we have the api endpoint from server
                # send_assets_to_xsiam(assets=assets)

            elif command == 'tenable-export-assets':
                results = get_assets_command(client, args)
                return_results(results)

        # Log exceptions and return errors
        except Exception as e:
            return_error(f'Failed to execute {demisto.command()} command.\nError:\n{str(e)}')


    ''' ENTRY POINT '''


    if __name__ in ('__main__', '__builtin__', 'builtins'):
        main()

    register_module_line('Tenable.io Event Collector Test', 'end', __line__())
  isfetchevents: true
  isfetchassets: true
  type: python
  subtype: python3
  dockerimage: demisto/python3:3.10.12.63474
  nativeimage:
  - '8.3'
  - '8.4'
fromversion: 6.8.0
tests:
- No tests (auto formatted)
marketplaces:
- marketplacev2
image: data:image/png;base64,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
detaileddescription: "## Configuration Parameters\n\n#### Access key and Secret key\nTenable.io generates a unique set of API keys for each user account (**access and secret key**). These keys allow your application to authenticate to the Tenable.io API without creating a session.\nThe method to generate API keys varies depending on the role assigned to your user account. Administrators can generate API keys for any user account. For more information, see Tenable.io Documentation.\n\n#### Vulnerabilities Fetch Interval\nTime in minutes between fetches of vulnerabilities (this is not exactly a fetch interval, but the time between each start of vulnerabilities fetching process). Be aware that it should be high (e.g., 240), as the process may take some time.\n\n#### Events Fetch Interval and Max Fetch\nThis interval affects only **Audit logs** fetch.\n\n\n---\n[View Integration Documentation](https://xsoar.pan.dev/docs/reference/integrations/tenableio-event-collector-test)"
